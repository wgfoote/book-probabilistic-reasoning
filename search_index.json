[
["index.html", "Probabilistic Reasoning Preamble This book Premises Don’t we know everything we need to know? What we desire A work in progress", " Probabilistic Reasoning William G. Foote 2020-09-30 Preamble This book is a compilation of many years of experience with replying to clients (CFO, CEO, business unit leader, project manager, etc., board member) and their exclamations and desire for the number. The number might be the next quarter’s earnings per share after a failed product launch, customer switching cost and churn with the entry of a new competitor, stock return volatility and attracting managerial talent, fuel switch timing in a fleet of power plants, fastest route to ship soy from Sao Paolo to Lianyungang, launch date for refractive surgery services, relocation for end state renal care facilities, and so on, and so much so forth. Here is highly redacted scene of a segment of a board meeting of a medical devices company in the not so distant past. All of the characters are of course fictions of a avid imagination. The scene and the company are all too real. Sara, a board member, has the floor. Sara: Bill, just give us the number! Bill is the consultant that the CFO hired to provide some insight into a business issue: how many product units can be shipped in 45 days from a port on the Gulf of Mexico, USA to a port in Jiangsu Provice, PRC. Bill: Sara, the most plausible number is 42 units through the Canal in 30 days. However, … Sara interjects. Sara: Finally! We have a numbe … Harrumphing a bit, but quietly and thinking “Not so fast!”, Bill: Yes, 42 is plausible, but so are 36 and 45, in fact the odds are that these results, given the data and assumptions you provided our team, as well as your own beliefs about the possibilities in the first place, are even. You could bet on any of them and be consistent with the data. Jeri the CFO grins, and says to George the CEO in a loud enough whisper so that other board members perked up from their trances, Jeri, CFO: That’s why we hired Bill and his team. He is giving us the range of the plausible. Now it’s up to us to pull the trigger. Sara winces, yields the floor to Jeri, who gently explains to the board, Jeri: Thank you Bill and give our regards to your team. Excellent and extensive work in a very short time frame!. Bill leaves the board room, thinking “Phew!, made through that gauntlet, again.” Jeri: Now our work really begins. George, please lead us in our deliberations. Pan through the closed board room door outside to the waiting room and watch Bill, wiping the sweat from his brow, prepare the invoice on his laptop. A watchful administrative assistant nods his approval. This book This book attempts to recast and reimagine the Bayesian STAN analytics presented by Richard McElreath (2020) for the data analytics that help business managers and executives make decisions. The data for the models is wrangled through the R tidyverse ecosystem that Solomon Kurz uses in recasting McElreath’s text with the brms R package developed by Paul Bürkner’s brms package. The concepts, even the zeitgeist of a Danielle Navarro might be evident along with examples from her work on learning and reasoning in the behavioral sciences. And if there was ever a pool of behavior to dip into it is in the business domain. Premises The one premise of this book is that Learning is inference By inference we mean reaching a conclusion. Conclusions can be either true or false. They are reached by a process of reflection on what is understood and by what is experienced. Let’s clarify these terms in this journey courtesy of Potter (1994): Ignorance is simply lack of knowledge. Opinion is a very tentative, if not also hesitant, subject to change, assent to a conclusion. Belief is a firm conviction. Knowledge is belief with sufficient evidence to justify assent. Doubt suspends assent when evidence is lacking or just too week. Learning can only occur with doubt or ignorance or even opinion; learning generates new knowledge. Don’t we know everything we need to know? We don’t seem to know everthing all of the time, although we certainly have many doubts and opinions. These hurdles are often managed through the collection of data against hypotheses, explanations, theories, and ideas. This syllogism and its sibling will guide us through a lot of the dark places we might travel in this book. Table 0.1: modus ponens plausibility major if A is true, then B is true minor B is true conclusion therefore, A becomes more plausible There is no guarantee here, just plausible, but justified, belief. We will call plausibility a measure of belief, also known as probability. Here is the sibling. Table 0.2: modus tollens plausibility major if A is true, then B is true minor A is false conclusion therefore, B becomes less plausible The first chapter will detail the inner workings and cognitional operations at work in these inferences. But let us remember that learning is inference. What we desire These two ways of inferring plausible, justified, true belief will be the bedrock of this book. They imply three desiderata of our methods: Include both the data of sensible observation and the data of assumptions and beliefs. Condition ideas, hypotheses, theories, explanations with the data of experience and belief. Measure the impact of data on hypotheses using a measure of plausiblity. What we will call rational will be the consistency of data with hypotheses measured by plausiblity, to be called posterior probability. The data of beliefs will be contained in what we will call prior probability. The conditioning of hypotheses with data is what will call likelihood. Ultimately what we call uncertainty will be the range and plausibility of the impact of data on hypotheses. What we will solve for is the most plausible explanation given data and belief. A work in progress This book will expand and contract over the days ahead. Email me will comments, errors, omissions, etc. The book’s content is rendered using Yihui’s bookdown package (thank you!), served on GitHub pages with source code. When I figure out how to build an issues facility in Github that will be the font of all wisdom from readers and syndics. References "],
["part-i-background.html", "Part I. Background", " Part I. Background Getting our arms around The logic of probability for inference An initial use case with binary data Information and its measurement Another use case of a Bayes relationship "],
["plausibility-probability-and-information.html", "Chapter 1 Plausibility, Probability and Information 1.1 Some Surprise 1.2 Informative? 1.3 We have a proposition for you 1.4 The power of plausible thinking 1.5 How many ways? 1.6 How informative? 1.7 Nota Bene", " Chapter 1 Plausibility, Probability and Information According to Aristotle, if two claims are well-founded, their truth values can be ascertained. A plate is either green, or it is not. When thrown, the plate will land without a chip or it will break. If I claim that when I throw the plate, it will land with out a chip and you disagree, I can simply throw the plate to find out who was correct. One part of this is a mind game, a thought experiment a potential outcome. The other is a reality of actually throwing the plate and observing its status on landing. We will eventually call the mind-game a hypothesis and the reality a datum. It is in disagreement that logical deduction might (plausibly) break down. There is no guarantee that the plate will break, or, for that matter, that it will chip. We must simply experiment with plate(s), green, blue or otherwise, to support the claim (or not). These claims arise in everyday life. For example, despite my poor performance in plate throwing in the past, there is no cogent reason to believe that it is absolutely, positively false that the plate I throw would land without a chip. There is a degree of acceeptance, of plausibility, in one side of the claim, and on the other as well. Certainly it is not as false as the claim that \\(2+2=5\\) in base 10 arithmetic, or the patently spurious claim that true is false or my cat is a dog. Claims about things that are neither definitely true nor definitely false arise in matters both mundane and consequential: producing weather reports, catching the bus, predicting the outcomes of elections, interpreting experimental vaccine results, and betting on sports games, throwing the plate, to name just a few. So we would benefit from a method of comparing claims in these situations – which atmospheric model produces better predictions? What is the best source for predicting elections? Should I blow three times on my lucky dice, or is this all just a figment of my denial based imagination? 1.1 Some Surprise The goal, in all the cases above, is to guess about something that we don’t or can’t know directly, like the future, or the fundamental structure of the economy, or reasons why customer preferences change, on the basis of things we do know, like the present and the past, or the results of an existing or past experiment. Mostly we guess. Some us try to systematically consider and attempt to support with evidence the guess. Lacking precision, and sometimes even accuracy, we try to avoid bad surprises. Goods ones are often welcome. If I use the NOAA weather application on my smart phone it might not surprise me to see rain pelting down at 1630 this afternoon. After all the app indicated as much. In advance of the rain I brought in chair pads and anything else that might get ruined with rain. An airline pilot knows all the defects of her aircraft. That knowledge saves lives. Our mortal inferences, clever or dumb as they are, must have a surprise somewhere between totally expected, or zero surprises and thus certain 100% of the ways to make the statement, and totally surprising and 0% chance of anticipation. We will generally be making statements like: it will probably rain tomorrow, or nine times out of ten, the team with a better defense wins. This motivates us to express our surprise in terms of plausibility and we hanker for more precision with probability. 1.2 Informative? When we producing clarity in the various interrleted concepts of any discourse, argument, report, or presentation of results in the process of analysis. The process begins with clearly definitions of data, methodology, assumptions, conclusions, and interpretations. Statements encapsulate the products of analysis. We define a statement, a literal, as a linguistic construct at a minimum consisting of a noun and verb, that is, a subject and a predicate. We might add an object, also a noun, too. If we agree that analysis produces knowledge, and knowledge is justified true belief,1 we will argue only when we dispute the (plausible / probable) true or false judgment. The dispute itself derives ultimately from what is valuable to discuss (Lepore and Stone (2006)). The practice of analysis is then the raising and answering of questions for reflection about the meaning of statements of results emanating from the analysis.2 We could use possible, probable, reasonable, and more generally if possible, plausible as in appearing worthy of belief. Let’s start with Aristotle’s apodeixis (Gr., showing forth; a proof as demonstrative argument) found in Aristotle (n.d.), especially the books on prior and posterior analytics. Ultimately we ask: do we have enough of the right information to make a credible judgment? If so, are we also willing to implement the implications of a decision based on justified true belief itself? Here we must define information. The word information itself derives from what is inside ( in- ) a shape ( form ) and more broadly an idea.3 The air molecules inside a blown up balloon is analogous to data, facts as we might see, hear, smell, and store. The balloon (please prescind, using a willing suspension of disbelief, from the material a balloon is made) shapes the air molecules, taken together in the context of elastic enclosure, into the shape of the expanded balloon. The shape of the balloon expanded such that the volume of air molecules expands the balloon, is analogous to the idea that shapes data. We will see that information only occurs when someone has that aha! moment. It occurs when we experience the unanticipated. It happens when we are surprised by an outcome. At its base value, information is then the unfolding of uncertainty. The shape of data is often mentioned in statistics where it means producing a frequency distribution of ranges of data from low to high values. What the frequency distribution does to data is order the data, bin and group the data into low to high categories, and count the number of low to high data elements. All of that shaping of the data is the application of ideas (order, groups, frequencies) about data. So how much information do we need (plausibly)? We can follow the discussion about the English language in Feynman and Allen (1996) and Edwin T. Jaynes (2004). Information, like mass, velocity, volume, can be measured. Its measure is the bit . Let’s start with the English language. We have 26 letters and punctuation (e.g., . , ? ! : ;)for a total of 32 symbols at our disposal. Of course we do not use all 32, but we do have them available to us. For the moment we have \\(n\\) equally plausible messages to send. One message might be a tweet line Got up and has some breakfast and another Didn’t do my chores yesterday. Assign each message a number of bits \\(I\\) needed to send the message. We might recall a bit is a binary set of numbers, 0’s and 1’s. Binary means two. The first part of the encoded message might be a 0, the second might be a 0 or a 1, the third might be a 0 or 1 following the 0 and following the 1, and so on. For example we might code this. \\(\\text{Got up and has some breakfast} = 0111000100\\) This is now easy enough to send in packets of on or off voltages across wireless networks. If we need \\(I\\) bits to send the message, then we need \\(2^I = n\\) or, remembering some long-lost algebra about extracting exponents, \\(I = log_2n\\) The 2 is because we have two possibilities for coding at each stage of coding our message. The n are the number of messages to be coded, send, and decoded by our communication system. If we have 10 messages to send, then we need \\(I = 3.32\\) bits, really four bits to do the job. That is the way we will characterize information about messages. Suppose our language has \\(xN\\) symbols to send and receive. If \\(N=32\\) is the number of symbols for one message segment in English, then for a string of \\(x=2\\) segments, say \\(us\\) for example, or \\(we\\), or \\(or\\), we have 64 symbols we can use the convey the message. Hold on for dear life now! there are now \\(n^2\\) messages we can send in a two string message. If there are \\(x=3\\) messages like \\(are\\) or \\(not\\), there are \\(xN=3\\times 32= 96\\) possible symbols in \\(n^3\\) possible messages and so on. We now have the drift. There are \\(n^x\\) possible messages to send, still all equally plausible. Exhaustingly, what is the information now in bits if we area sending and receiving 5 messages?4 \\(I(xN)=log_2(n^x)=xlog_2(n)= 3 \\times log_2(5) = 7 \\,\\, bits\\) If we think of a bit as space on a card that carries the messages, we get an idea about the amount of information in the message. More information means more bits and takes up more space on the card. Space on the card is real. Bits are a measure we use as a convenience to talk about information on a physical medium like a card. Now that we have the beginnings of a notion of information, quasi-quantiatively, we need to tackle plausibility. We said that each message is equally plausible to be on or off, true or false, and yes, this means we have to think about logic, which worries itself over the business of combining the truth values of statements. 1.3 We have a proposition for you We move from information now; and just put that on hold for a bit (snarky, yes). Its well nigh time to develop the most basic algebra possible, the logic of George (???). We follow Polya (1954) and Edwin T. Jaynes (2004) in notation. The logical product or conjunction is a both-and proposition. If \\(A=1\\), that is \\(A\\) is true, and \\(B=1\\), that is \\(B\\) is also then the proposition that both A and B must also be true. This proposition is symbolized with \\[ AB \\] and substituting the assigned truth values of \\(1 = true\\) and \\(0 = false\\), we can then in effect calculate that \\[ AB=(1) \\times (1) = 1 = true \\] Thus if \\(A\\) and \\(B\\) are true then taken together as both A and B is also true. The logical sum or disjunction is an either-or proposition. If \\(A=1\\), that is \\(A\\) is true, and \\(B=0\\), that is \\(B\\) is false then the proposition that either A or B must be true. This proposition is symbolized with \\[ A + B \\] and substituting the assigned truth values of \\(1 = true\\) and \\(0 = false\\), we can then in effect calculate that \\[ A + B=(1) + (0) = 1 = true \\] And also with these truth values \\[ AB=(1) \\times (0) = 0 = false \\] We can see then that conjunction and disjunction can yield diametrically opposed conclusions about the truth value of the two compound statements. To deny a proposition we use the bar, read not A So if \\(A=1=true\\), then \\(not\\,\\,A = \\bar{A} = |1 - 1| = false\\). Also if \\(A=0=false\\), then \\(not\\,\\,A = \\bar{A} = |0 - 1| = 1 = true\\), where the vertical bars represent the absolute value operation needed to get the right logical answer from the binary arithmetic. Let’s play with this. What is \\(\\bar{AB}\\), again using the binary arithmetic of combining \\(0\\) and \\(1\\)? What about the plausibility of \\(A|BC\\)? This is read the _plausibility of \\(AB\\) given, conditional upon, experienced with \\(C\\). The experience of \\(C\\) will allow us to include that which is fundamental to all decision making, the fact of a history and knowledge of events and their outcomes, including the experience of beliefs and changes in preferences to go along with anticipating the future of anything. It turns out that the basic idea of conditioning, making assumptions, or creating implications of the form If A happens, then B happens is equivalent to the statement that if A does not happen, or B does happen. Here is an implication truth table to whet our burgeoning appetite for analysis. Table 1.1: implication truth table P Q if P then Q TRUE TRUE TRUE TRUE FALSE FALSE FALSE TRUE TRUE FALSE FALSE TRUE We can, sometime in the near future, of course check that the results of this table agree with the statement that not P or Q. Knowing this we can begin to argue – reasonably. 1.4 The power of plausible thinking We talked about information, and some logic. Now we have the machinery to discuss plausibility. We recall that all messages were equally likely. We begin relax that assumption her and now. Let’s begin with any two propositions or statements or messages \\(A\\) and \\(B\\). Specifically, let’s talk about the weather. We let \\[ \\begin{align} A &amp;= \\text{It will start to rain by noon at the latest right here.} \\\\ B &amp;= \\text{The sky will get cloudy before noon, also right here.} \\end{align} \\] The implication is the statement If A, then B. This is our starting point with plausibility. But we must first begin even more strongly with a guaranteed statement from the following deduction. If it will rain, then it must happen that the sky will get cloudy some nearly immediate time before it does rain, right here on the sphere of the earth. Table 1.2: modus ponens deduction major if A is true, then B is true minor B is true conclusion therefore, A is true Here is a Euler Diagram to help make the point. The full name of this syllogism is modus ponendo ponens which is Latin for the way that, by affirming, affirms , also known as affirming the antecedent . Take the word plausible out of the syllogism and we have a deductive inference guarateed to be consistent with the logic of implication.5 Here is a truth table for modus ponens. Each column holds the truth values ( TRUE = T and FALSE = F) of each statement in the syllogism. Table 1.3: modus ponens truth table P Q if P then Q P Q TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE TRUE FALSE FALSE TRUE TRUE FALSE TRUE FALSE FALSE TRUE FALSE FALSE We can read this table line by line using this statement: If you get an A, then I’ll give you a lollipop. The statement will be true if I keep my promise and false if I do not. Suppose it is true that you get an A and it is true that I give you a lollipop. Since I kept my promise, the implication, the statement, is true. This corresponds to the first line in the table. Doesn’t this sound like a logical conjunction? Both A and B are true, that is \\(AB = (1) \\times (1) = 1 = TRUE\\). Suppose it’s true that you get an A but it’s false that I give you a dollar. Since I didn’t keep my promise, the implication is false. This corresponds to the second line in the table. These truth values again act like a logical conjunction. If A is true and B is false, we have \\(AB = (1) \\times (0) = 0 = FALSE\\). and also line 4. What if it is false that you get an A? Whether or not I give you a lollipop (Q is true or Q is false), I have not broken my promise. Thus, the implication cannot be false, so the statement must be true. This case is no longer a simple logical conjuction, but we can use the negation to help us out with the idea that If A is false it is not-A, when A would otherwise be true. The conjunction \\(\\overline{A}B = 0 \\times 1 = 0 = FALSE\\). Now we use (???) Prior and Posterior Analytics as our starting point. But insteadof the logically guaranteed modus ponens we weaken the guarantee in a syllogism called epagoge6 Table 1.4: modus ponens plausibility major if A is true, then B is true minor B is true conclusion therefore, A becomes more plausible This is a variation on the classical modus ponens consistent argument from finding positive evidence. Here the evidence is that \\(B\\) is true. This evidence does not guarantee that \\(A\\) is also true, but does lend credence, plausibility, to the existence of \\(A\\). So this morning it is true that the sky is cloudy. It is also true that when the sky was cloudy we can expect it to rain. It still might not rain if a strong wind dissipates the clouds; but it is plausible. Let’s try a variation on the theme of modus ponens, the consistent argument from the absence of evidence aptly named modus tollens .The full name is modus tollendo tollens which is Latin for the way that, by denying, denies , also known as denying the consequent. Table 1.5: modus tollens deduction major if A is true, then B is true minor A is false conclusion therefore, B is false Insert the word plausibility again. This maneuver will weaken the modus tollens guarantee.7 Table 1.6: modus tollens plausibility major if A is true, then B is true minor A is false conclusion therefore, B becomes less plausible The evidence that \\(A\\) is false does not prove that \\(B\\) is also false. In this case we eliminated a piece of evidence and thus are even less confident in our hypothesis that \\(B\\) might be true. In our everyday life we confront situations like this. Suppose that I stand on a poorly lit street and observe a masked very fit looking person of combat age is running, almost tripping, down the steps of the 6 train elevated (open-air) subway station at Middletown Road carrying a large tote bag. At the same time I hear the train platform announcement to beware of pickpockets. I conclude that the person is very plausibly a pickpocket. I am about to reach for my phone to call the police. I believed in that moment that my plausible argument had the power of the strongest of deductive reasoning, namely that the conclusion was beyond the shadow of any doubt.8 The use of modus tollens will allow us to think through how to find probabilities of conditional events using Bayes Theorem and the ability to update our assessment with the rule for total probability all logically consistent. 1.5 How many ways? Let’s use a simple example. We have four voters in an upcoming election. They may be red or blue voters. Three of us go out and talk to three voters at random, that is, indiscriminately. One of us happens to come upon a blue voter, another of us, independently, happens to find a red voter, and the other separately finds a blue voter. This is the very definition of a random sample. Each of the finders does not know what the other is doing, all three do know that there are four voters out there and they happened to have independently talked to two blue and one red voter. How many red voters and how many blue voters are there? Here are all of the possible conjectures we can make for \\(blue = {\\color{blue}\\Box}\\) and \\(red = {\\color{red}\\Box}\\) voters. Table 1.7: voter conjectures 1 2 3 4 \\({\\color{red}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{blue}\\Box}\\) Reading this we see that there are 4 voters and 5 different voter compositions ranging from all red to all blue. Our sample is 2 blue and 1 red voter, so we can very safely eliminate the first and fifth conjectures from our analysis, but for the moment just keep them for completeness sake. For each of the three remaining conjectures we may ask how many ways is the conjecture consistent with the collected data. For this task a tree is very helpful. Let’s take the first realistic conjecture the \\({\\color{blue}\\Box}\\), \\({\\color{red}\\Box}\\), \\({\\color{red}\\Box}\\), \\({\\color{red}\\Box}\\) hypothesis and check if, when we sample all of the four voters, what are all of the ways this conjecture fans out. So here we go. We sampled a \\({\\color{blue}\\Box}\\) first. How many \\({\\color{blue}\\Box}\\)’s are in this version of the composition of voters? only 1. We then sampled independently a \\({\\color{red}\\Box}\\). How many \\({\\color{red}\\Box}\\)’s are in this conjecture? Quite a few, 3. Finally we sampled a \\({\\color{blue}\\Box}\\) at random. We know there is only one \\({\\color{blue}\\Box}\\) in this version of the truth. So, it is just counting the ways: 1 \\({\\color{blue}\\Box}\\) way x 3 \\({\\color{red}\\Box}\\) ways x 1 \\({\\color{blue}\\Box}\\) way = \\(1 \\times 3 \\times 1 = 3\\) ways altogether. When asked, many surmise that the 2 blue and 3 red conjecture is the right one. Are they right? Here is a table of the ways each conjecture pans out. We then in a separate column compute the contribution of each conjecture to the total number of ways across the conjectures, which is 3 + 8 + 9 = 20 ways. Also each of the conjecture propose a proportion \\(p\\) of the successes, that is, the blue voters in this context. Table 1.8: ways voter conjectures turn out 1 2 3 4 proportion ways plausibility \\({\\color{red}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{red}\\Box}\\) 0.00 0 x 4 x 0 = 0 0.00 \\({\\color{blue}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{red}\\Box}\\) 0.25 1 x 3 x 1 = 3 0.15 \\({\\color{blue}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{red}\\Box}\\) 0.50 2 x 2 x 2 = 8 0.40 \\({\\color{blue}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{red}\\Box}\\) 0.75 3 x 1 x 3 = 9 0.45 \\({\\color{blue}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{blue}\\Box}\\) 1.00 4 x 0 x 4 = 0 0.00 We cannot help but note that the proportion of ways for each conjecture can range from 0, perhaps to 1, since the proportions add up to 1. The number of ways also expresses the number of true consistencies of the data with the conjecture, an enumeration of the quality of the logical compatibility of conjectures with what we observe. We might now revise our common sensical surmise that 2 blue and 2 red is the better conjecture. However, if we use the criterion that the conjecture with the most ways consistent with the data is the best choice for a conjecture, then clearly here we would say that there are 3 blues and 1 red. Perhaps we have a better criterion that would choose our equinanimous choice of 2 blues and 2 reds? It does not appear to be so. Ways are the count, the frequencies of logical occurrence of a hypothesis given the data. The data includes the knowledge that there are possibly blues and reds, that there are 4 voters, and that we sampled 2 blues and 1 red. The relative frequency of the ways in which our conjectures are consistent with the data is what we will finally call probability. The plausibility is a measure between and including 0 and 1. The sum of all plausibilities is 1. We have just quantified the plausibility of logical truth values. We have also found a very compelling criterion for the choice of a conjective given the data and circumstances surrounding our inquiry. 1.6 How informative? Now the piece de resistance we have been waiting for is at hand. We have the tools: information measurement, logic of plausible inference, probability as plausibility, a criterion for making a decision? If we were to say, pick the most plausible, now also the most probable inference, is this not really to say that it is the most informative and thus the most surprising inference? We have \\(i=5\\) conjectures \\(c_i\\), each with an assignment of the probability of finding the inference in data, thus 5 probabilities \\(p_i\\). We also agree that each conjecture \\(c_i\\) is independent of the other, that is mutually exclusive, and also mutually exhaustive. In this way the probabilities indeed will add up to 1. Each conjecture has \\(N=4\\) symbols in it. There are only 2 symbols to work with, \\(B=blue\\) and \\(R=red\\) in these messages. There are \\(N!=4 x 3 x 2 x 1 = 24\\) ways to arrange symbols. But these are constrained by two symbols in this pile so we have on average this many combinations: \\(\\frac{(N + s - 1)!}{N!(s-1)!} = \\frac{5!}{4!1!} = \\frac{120}{24}=5\\,\\,conjectures\\) In the numerator are the four voters, two kinds of voters, less one. This gives us \\(5!=120\\) total ways to arrange this set of conjectures. But there are \\(4!(2-1)!=\\) ways to pick 2 kinds of voters from a population of 4 voters. That comes from the notion that there are \\(4!\\) ways to choose 4 voters and having chosen the first voter from the voter urn, there are \\((2-1)!\\) more ways to choose voters, either blue or red, each and every time we randomly go out to the voting population, all with replacement. Phew! Now that we have our sea legs in combinatorics, each conjecture of length \\(N=4\\) has two symbols \\(s_1=blue\\) and \\(s_2=red\\) with probabilities of occurrence in the message \\(p_1=p(blue)\\) and \\(p_2=p(red)\\) here \\(0.50\\) each. The portion of the messages that are blue are \\(Np_1=4 \\times 0.5=2\\) and red are \\(Np_2=4 \\times 0.5=2\\) as well. The ratio \\(\\frac{N!}{(Np_1)!(Np_2)!} = \\frac{4!}{2!2!}=\\frac{24}{(2)(2)}=6\\) in turn gives us the number of combinations of symbols and messages. This is the average number of ways a message can be formed. The expected amount of information in these messages, as conjectures, is \\(&lt;I&gt; = log_2\\left(\\frac{N!}{(Np_1)!(Np_2)!}right)\\) For large enough \\(N\\) we can use Stirling’s Approximation9 \\(&lt;I&gt; = N \\sum_{i=1}^2 (-p_i\\,log_2 \\,p_i)\\) So the amount of informativeness in each voter conjectures is \\(&lt;I&gt; = 4 \\sum_{i=1}^2 (-0.5 \\,log_2 \\,0.5) = 4\\) This is intuitively correct: 4 bits of information in a population of 4 with 2 equally likely choices. What about the 5 conjecture? Now \\(N=5\\) and we only need \\(p_2=0.15, p_3=0.40, p_4=0.45\\) since the other probabilities are 0. \\(&lt;I&gt; = 4 \\sum_{i=1}^3 (-p_i \\,log_2 \\,p_i) = 4(1.5)=5.8\\) This is nearly 2 bits more informative than any single conjecture. The best conjecture contributes 2.1 bits of information on its own or 36% of the informativeness of the total distribution of conjectures we reviewed. 1.7 Nota Bene The location and scale parameters \\(\\mu\\) and \\(\\sigma\\) we often calculate are not the mean and standard deviation of data \\(x\\). They are not properties of the physical representation of events called data. These parameters do carry information about the probability distribution of the representation of physical reality in data. To say \\(\\mu\\) is the mean of the data is to ascribe a mind’s eye idea to the physical reality, to invest in physical, objective reality, a property that only exists in the mind. This is an example of the mind projection or reification fallacy much in vogue in the circles of fake, or better yet, chop logic. In the same way, probabilities only exist in our minds: there is no physical reality that is a probability, just a mental construct that helps us think through potential outcomes. So why is it that we so much so rely on the past to predict the future? Perhaps this is a fool’s errand. I When we say A implies B we mean that \\(A\\) and \\(AB\\) have the same truth value. In logic every true statement implies every other true statement. Just knowing that \\(A\\) and \\(B\\) are true does not References "],
["probability-for-real-people.html", "Chapter 2 Probability for Real People 2.1 Can we rationally reason? 2.2 What’s next?", " Chapter 2 Probability for Real People 2.1 Can we rationally reason? Rationality here at least means that we, as decision makers, would tend to act based on the consistency of observed reality with imagined and through ideas about the world in which data are collected. We attempt to infer claims about the world based on our beliefs about the world. When confronting ideas, imbued with beliefs, with ovbserved reality we might find ourselves in the position to update our beliefs, even those, and sometimes especially those, we so dearly hold. In our thinking about anything we would venture candidate hypotheses \\(h\\) about the world, say the world of voters in voting districts, consumers of smart phones in zip codes, even virus testing results. Of course the whole point is that we do not know which hypothesis is more plausible, or not. We then collect some data \\(d\\). When we perform this task, we move from the mental realm of the possibiity of hyptheses, theories, surmises, and model to the realm of observed reality. We may well have to revise our original beliefs about the data. To implement our maintained hypothesis of rationality, we begin our search for potential consistencies of the collected data with our hypotheses that are fed by the data. In our quest we might find that some one of the hypotheses has more ways of being consistent with the data than others. When the data is consistent with a hypothesis, that is, when the hypothesis is reasonable logically, then our belief in that hypothesis strengthens,10 and becomes more plausible. If the data is less consistent with the hypothesis, our belief in that hypothesis weakens. So far we have performed this set of tasks with conjectures about virus testing and voter alliance in zip codes. Let’s switch up our program and consider the following very simplified question about the weather. We see people carrying snow shovels. Will it snow? What is the data \\(d\\)? We have recorded a simple observation about the state of the weather so that single piece of data (\\(d =\\) We see people carrying snow shovels). Her is where our beliefs enter. We have two hypotheses, \\(h\\): either it snows today or it does not. Let’s figure out how to solve this problem? We have three desiderata: We should include our experiences with snow in our analysis. We should collect data about carrying snow shovels in January as well. We prefer more consistency of data with hypotheses to less consistency. Here we go, let’s strap ourselves in. 2.1.1 Priors: what we think can happen Our observation is about the weather: clouds, wind, cold. But we want to know about the snow! That is our objective and we have definite ideas about whether (don’t pardon the pun!) it will snow or not. We will identify our beliefs, ever before we make our observations, about snow. The analytical profession and custom is to label these beliefs as a priori,11 and thus the ellipsis prior, contentions we hold when we walk into the data story we create with the question of will it snow? After all we have to admit to everyone what we believe to be true as the antecedent to the consequent of observations and the plausibility of snow. This move allows us to learn, to revise, to update our dearly held beliefs. We thus can grow and develop. This is in a phrase a sine qua non, a categorial imperative, a virtually unconditioned requirement for change. What might we believe about whether it will snow (today)? If you come from Malone, New York, north of the Adirondack mountains, you will have a different belief than if you come from Daytona, Florida, on the matter of how many ways snow might happen. So let’s take as our benchmark Albany, the capital of the state of New York using this weather statistics site. The site reports the average number of days of snowfall in January, when there is at least a 0.25 cm accumulation in a day. It is 10.3 days. These are the number of ways (days) in January, in Albany, NY, that it is true, on average and thus some notion of expected, or believed to be, that it snows. The total number of ways snow could possibly fall in any January (defined by calendar standards) is 31. Thus we might conclude that we believe that is it plausible (probable) that snow can fall \\(10.3 / 31 = 30\\%\\) of the different ways snow can fall. Note very well we will talk about priors as potentials, and thus used the modal verb can. Then that means that we believe it might not snow with plausibility \\(1-0.30 = 0.70\\), or, multiplying by 100, 70%, according to the law of total probability of all supposed (hypothesized) events. Table 2.1: Priors by hypotheses hypotheses priors Snow day 0.3 Nice day 0.7 Nice ideas, nice beliefs, but how real, how plausible, how rational are they? 2.1.2 Likelihoods: thinking about the data Life in the Northeast United States in January much revolves around the number of snow days, also known as days off from school. A prediction of snow meets with overtime for snow plow drivers, school shut downs, kids at home when they normally are in school. On some snowy days we see people carrying snow shovels, on others we don’t. On some nice days we see people with snow shovels, on others we don’t. Confusing? Confounding? A bit. Now we link shovels with snow. We then suppose we observe that people carry snow shovels about 7 of the 10 snowy days in January or about 70%. On nice days we observe that people carry shovels at most 2 days in the 21 nice days or about 10%. This table records our thinking using data we observe in Januaries about weather conditions. Table 2.2: data meets hypotheses hypotheses shovels hands snow day 0.7 0.3 nice day 0.1 0.9 First of all these probabilities register yet another set of beliefs, this time about whether we see shovels or not, given, conditioned by, the truth of each hypothesis \\(h\\). We write the conditional probability \\(Pr(d|h)\\), which you can read as “the probability of \\(d\\) given \\(h\\)”. Also here we will follow the convention that this set of results of our assessment of the relationship of shovels to snowy days as a likelihood .12 2.1.3 Altogether now Do we have everything to fulfill our desiderata? Let’s check where we are now. We should include our experiences with snow in our analysis. Yes! We put our best beliefs forward. We even (sometimes this is a courageous analytical step) quantified teh ways in which snow and not snow would occur, we believe, in Albany NY in an average January.{^january} We should collect data about carrying snow shovels in January as well. Yes we did! Again we elicited yet another opinion, belief, whatever we want to colloguially call it. That belief if what we register and docuement based on observation of shovels and just hands in the presence of snowy and nice days in a January. We prefer more consistency of data with hypotheses to less consistency. Not yet! We will impose our definition of rationality here. Let’s start out with one of the rules of probability theory. The rule in question is the one that talks about the probability that two things are true. In our example, we will calculate the probability that today is snowy (i.e., hypothesis \\(h\\) is true) and people carry shovels (i.e., data \\(d\\) is observed). The joint probability of the hypothesis and the data is written \\(Pr(d,h)\\), and you can calculate it by multiplying the prior \\(Pr(h)\\) by the likelihood \\(Pr(d|h)\\). Logically, when the statement that both \\(d\\) and \\(h\\) is true, then the plausibility, now grown into probability is: \\[ Pr(d \\wedge h) = Pr(d|h) Pr(h) \\] When we divide both sides by \\(Pr(h)\\) we get the definition, some say derivation, of condition probability. If we count \\(#()\\) the ways \\(d \\wedge h\\) are true and the ways that \\(h\\) are true then \\[ \\#(d|h) = \\frac{\\#(d \\wedge h)}{\\#(h)} \\] Then the number of ways the data \\(d\\) are true, given \\(h\\) is true, equals the total number of ways that \\(d\\) and \\(h\\) per each way that \\(h\\) is true. We have thus normed our approach to understanding a conditional statement like if \\(h\\), then \\(d\\). Even more so, when we combine the law of conditional probability with the law of total probability we get Bayes Theorem. This allows us to recognize the dialectical principle that, yes, we recognize \\(h = snowy\\), but we also know that every cloud has its silver lining and that there is a non-snowy day and thus a \\[ not\\,\\,h = \\lnot h = nice \\] lurking in our analysis. Here it in in all its glory. \\[ Pr(h\\mid d) = \\frac{Pr(d\\mid h)\\,Pr(h)}{Pr(d\\mid h)\\,Pr(h)+Pr(d\\mid \\lnot h)\\,Pr(\\lnot h)} \\] The numerator is the same as the conjunction both \\(d\\) and $h. The denominator is the probability that either both \\(d\\) and \\(h\\) or both \\(d\\) and \\(h\\) are true. While the build up to this point is both instructive, and thus may at first be confusing, it is useful as will highlight the roles these probabilities perform in the drama that is our analysis. We had better get back to the data or get lost in the weeds of math. So, what is the probability it is true that today is a snowy day and we observed people to bring a shovel? Let’s see what we already have. Our prior tells us that the probability of a snowy day in any January is about 30%. Thus \\(Pr(h) = 0.30\\). The probability that we observe people carrying shovels is true given it is a snowy day is 70%. So the probability that both of these things are true is calculated by multiplying the two to get 0.21. We can make this \\[ \\begin{array}{l} Pr(snowy,\\,shovels) &amp; = &amp; Pr(shovels \\, | \\, snowy) \\times Pr( snowy ) \\\\ &amp; = &amp; 0.70 \\times 0.30 \\\\ &amp; = &amp; 0.21 \\end{array} \\] This is an interesting result, something odds makers intuitively know when punters put skin in the game. There will be a 21% chance of a snowy day when we see shovels in people’s hands. However, there are of course four possible pairings of hypotheses and data that could happen. We then repeatthis calculation for all four possibilities. We then have the following table. Table 2.3: both data and hypotheses hypotheses shovels hands sum snow day 0.21 0.09 0.3 nice day 0.07 0.63 0.7 sum 0.28 0.72 1.0 Just to put this into perspective, we have for the 31 days in a January this table. Table 2.4: both data and hypotheses in days in January hypotheses shovels hands sum snow day 6.5 2.8 9.3 nice day 2.2 19.5 21.7 sum 8.7 22.3 31.0 We have four logical possibilities. We arrange these possibilities in two stacked rows. We recall that visualizatiton is everything, even in tables! Here is the first row. Snowy and shovels \\[ \\begin{array}{l} Pr(snowy,\\,shovels) &amp; = &amp; Pr(shovels \\, | \\, snowy) \\times Pr( snowy ) \\\\ &amp; = &amp; 0.70 \\times 0.30 \\\\ &amp; = &amp; 0.21 \\end{array} \\] Snowy and just hands \\[ \\begin{array}{l} Pr(snowy,\\,hands) &amp; = &amp; Pr(hands \\, | \\, snowy) \\times Pr( snowy ) \\\\ &amp; = &amp; 0.30 \\times 0.30 \\\\ &amp; = &amp; 0.09 \\end{array} \\] In this row the prior probability about snow is 0.30. Here is the second row. Nice and shovels \\[ \\begin{array}{l} Pr(nice,\\,shovels) &amp; = &amp; Pr(shovels \\, | \\, nice) \\times Pr( nice ) \\\\ &amp; = &amp; 0.10 \\times 0.70 \\\\ &amp; = &amp; 0.07 \\end{array} \\] Nice and just hands \\[ \\begin{array}{l} Pr(nice,\\,hands) &amp; = &amp; Pr(hands \\, | \\, nice) \\times Pr( nice ) \\\\ &amp; = &amp; 0.90 \\times 0.70 \\\\ &amp; = &amp; 0.63 \\end{array} \\] In this row the prior probability about nice days is 0.70. A great exercise is to carry these calculations from the number of ways snow with and without shovels occurs given we think we know something about snow. The same with the number of ways a nice day might occur with and without shovels, given what we think about nice days. Let’s put one calculatin together with a not so surprising requirement. When we conjoin snow with shovels, how many possible ways can these logical statements occur? It is just the 31 days. We now have all of the derived information to carry our investigation further. We also total the rows and, of course, the columns. We will see why very soon. The row sums just tell us as a check that we got all of the ways in which snow occurs in 31 days. What is brand new are the column sums. They add up the ways that data occurs across the two ways we hypothesize that data can occur: snow, no snow (nice day). They tell us the probability of carrying a shovel or not, across the two hypotheses. Another way of thinking about the $p(d)4 column sums is that they are the expectation of finding snow or hands in the data. The consistency of all of these calculations is that column sums equal row sums, 100%. All regular, all present and correct, probability-wise. 2.1.4 Updating beliefs The table lays out each of the four logically possible combinations of data and hypotheses. So what happens to our beliefs when they confront data? In the problem, we are told that we really see shovels, just like the picture from Albany, NY at the turn of the 20th century. Is surprising? Not necessarily in Albany and in January, so you might expect this behavior out of habit during a rough Winter. The point is that whatever our beliefs have been about shovel behavior, we should still subject them to the possibility of accomodating the fact of seeing shovels in hands in Albany in January, a winter month in the Northern Hemisphere. We should recall this formula about the probability of seeing both an hypothesis and data: \\[ Pr(h \\mid d) = \\frac{Pr(d \\wedge h)}{Pr(d)}=\\frac{Pr(d \\mid h) Pr(h)}{Pr(d)} \\] Now we can trawl through about our intuitions and some arithmetic. We worked out that the joint probability of both snowy day and shovel is 21%, a rate reasonable given the circumstances. In our formula, this is the product of the likelihood \\(Pr(d=shovels \\mid h=snow)=0.70\\) and the prior probability we registered that snow might occur \\(Pr(h=snow)=0.30\\). Relative to the product of the likelihood of shovels given a nice day and the chance that snow might occur is the the joint probability of both nice day and shovel at 10%, or \\(Pr(d=shovels \\mid h = nice)Pr(h=nice)=0.10\\times 0.70=0.07\\), again a reasonable idea, since we plausibly wouldn’t see much shovel handling on that nice day in January.. Both of these estimates are consistent with actually seeing shovels in people’s hands. But what are the chances of just seeing shovels at all? This is an either or question. We see shovels 21% of the time on snowy days or we see shovels 7% of the total days in January on nice days. We then add them up to get 28% of the time we see shovels in all of January, whether it snows or not. So back to the question: if we do see shovels in the hands of those folk, will it snow? The hypothesis is \\(h=snow\\) and the data is \\(d=shovels\\). The joint probability of both snow and shovels is \\(Pr(d, h)=0.21\\). But just focusing on the data we just observed, namely that we see shovels, we now know that the chances of seeing shovels on any day in January in Albany, NY is \\(Pr(d)=0.27\\). Out of all of the ways that shovels can be seen in January then we would anticipate that the probability of snow, upon seeing shovels, must be \\(Pr(h \\mid d)=Pr(d,h)/Pr(d)=0.21/0.28=0.75\\). What is the chance of a nice day given we see shovels? It would be again likelihood times prior or \\(0.10\\times0.7=0.07\\) divided by the probability of seeing shovels any day in January 28%. We then calculate \\(0.07/0.28=0.25\\). We now have the posterior distribution of the two hypotheses, snow or nice, in the face of data, shovels. So what are the odds in favor of snow when we see shovels? \\[ OR(h \\mid d)=\\frac{Pr(h=snow \\mid d=shovels)}{Pr(h=nice \\mid d=shovels)}=\\frac{0.75}{0.25}=3 \\] We can read this as: when we see people with shovels in January in Albany, NY, then it is 3 times more plausible to have a snowy day than a nice day. The ratio of two posteriors gives us some notion of the plausible divergence in likely outcomes of snowy versus nice days. Again we must append the circumstances of time and place: in a January and in Albany, NY. Here is table that summarizes all of our work to date. Table 2.5: likelihood tempered by belief = posteriors hypotheses shovels hands priors posterior shovels posterior hands snow day 0.7 0.3 0.3 0.75 0.13 nice day 0.1 0.9 0.7 0.25 0.88 sum 0.8 0.2 1.0 1.00 1.00 2.2 What’s next? We have travelled through the complete model of probabilistic reasoning. We started with a question. The question at least bifurcates into the dialectical is it? or is it not?. We then began to think about beliefs inherent in the question for each of the hypotheses buried in the question. We then collected data that is relevant to attempting an answer to the question relative to each hypothesis. Then we conditioned the data with the hypotheses inside the question. It is always about the question! Finally we derived plausible answers to the question. What is next? We continue to use this recurring scheme of heuristic thinking, sometimes using algorithms to count more efficiently, applied to questions of ever greater comnplexity. In the end our goal will be to learn, and learning is inference. The core idea of strengthen is to take us from a more vulnerable to a less vulnerable place or state. Synonyms for strength include confirm and validate.↩ The a priori elements of any argument include just about everything you and I know, including the kitchen sink! We can’t help but to have these antecedent thoughts, experiences, shared and not-so-shared histories. They tend to persist in most humans, including us. At least that is what we will maintain. Thus it is a necessity to include these beliefs in our discussion. Without their consideration we most plausibly will introduce unsaid and denied bias, let blindspots have the same focus as clearly understood experiences, and produce time and resource consuming blind alleys. But we should hang on here: even blind alleys and blind spots are extremely important bits of knowledge that help us understand what does not work, an inverse insight as exposed by Bernard Lonergan (1970).↩ likelihood↩ "],
["part-ii-using-r.html", "Part II. Using R", " Part II. Using R R and RStudio set up R markdown and latex for text rendering Base R data and calculations Tidyverse wrangling and visualization "],
["warming-up-to-r-and-more.html", "Chapter 3 Warming up to R and more 3.1 Overview 3.2 What is R? 3.3 R for analytics 3.4 Hot and cold running resources 3.5 Installing R 3.6 First day at school 3.7 Install LaTex 3.8 Some fun with Stan 3.9 Github", " Chapter 3 Warming up to R and more 3.1 Overview In this chapter we will Discuss R for analytics Install R, RStudio, and Latex Install R markdown and run a document Have some fun with STAN 3.2 What is R? R is software for interacting with data along a variety of user generated paths. With R you can create sophisticated (even interactive) graphs, you can carry out statistical and operational research analyses, and you can create and run simulations. R is also a programming language with an extensive set of built-in functions. With increasing experience, you can extend the language and write your own code to build your own financial analytical tools. Advanced users can even incorporate functions written in other languages, such as C, C++, and Fortran. The current version of R derives from the S language. S has been around for more than twenty years and has been with extrensive use in statistics and finance, first as S and then as the commercially available S-PLUS. R is an open source implementation of the S language that is now a viable alternative to S-PLUS. A core team of statisticians and many other contributors work to update and improve R and to make versions that run well under all of the most popular operating systems. Importantly, R is a free, high-quality statistical software that will be useful as you learn financial analytics even though it is also a first-rate tool for professional statisticians, operational researchers, and financial analysts and engineers.\\footnote(But see this post on a truly big data language APL: https://scottlocklin.wordpress.com/2013/07/28/ruins-of-forgotten-empires-apl-languages/) 3.3 R for analytics There are several reasons that make R an excellent choice of software for an analytics course. Some benefits of using R include: R is free and available online. R is open-source and runs on UNIX, Windows, and Macintosh operating systems. R has a well-documented, context-based, help system enhanced by a wide, and deep, ranging user community globally and across several disciplines. R has excellent native static graphing capabilities. Interactive dynamic graphics are evolving along with the ability to embed analytics into online applications. With R you can build dashboards and websites to communicate results dynamically with consumers of the analytics you generate. Practitioners can easily migrate to the commercially supported S-Plus program, if commercial software is required. S and S-Plus are the immediate ancestors of the R programming environment. Cloud computing is now available with large data implementations. Microsoft now supports a commercial R version. R’s language has a powerful, easy-to-learn syntax with many built-in statistical and operational research functions. Just as important are the extensive web-scraping, text structuring, object class construction, and the extensible functional programming aspects of the language. A formal language definition is being developed. This will yield more standardization and better control of the language in future versions. R is a computer programming language. For programmers it will feel more familiar than for others, for example Excel users. R requires array thinking and object relationships that are not necessarily native, but indeed are possible, in an Excel spreadsheet environment. In many ways, the Excel style and R style of environments complement one another. Even though it is not necessarily the simplest software to use, the basics are easy enough to master, so that learning to use R need not interfere with learning the statistical, operational research, data, and domain-specific concepts encountered in an analytics-focused course.13 Doing statistics in a spreadsheet (e.g., Microsoft Excel) is generally a bad idea. While hundreds of millions of Excel users might disagree with this proposition learn the lesson of the 415 Report spreadsheet model to manage risks at JP Morgan Chase. Although many people are likely feel more familiar with them, spreadsheets are very limited in terms of what analyses they allow you do. If you get into the habit of trying to do your real life data analysis using spreadsheets, then you’ve dug yourself into a very deep hole. Proprietary software is expensive, not very extensible, and has many routines that are often opaque to users. When you have a chance look up the cost of a Matlab or SAS single user license. The tools you will need will be provided a la carte at a price for licensing fees. R is highly extensible. When you download and install R, you get all the basic “packages”, and those are very powerful on their own. However, because R is so open and so widely used, it’s become something of a standard tool in statistics, and so lots of people write their own packages that extend the system – all freely available. R is a real programming language. As you get better at using R for data analysis, you’re also learning to program. When you program you must think through the question you are posing, the data you are collecting, the analytical techniques you will deploy, the visualization of your results. This workflow is commonly called the software development lifecycle. If you don’t already know how to program, then learning how to do statistics using R is a good way to begin. There is at least one drawback. The primary hurdle to using R is that most existing documentation and plethora of packages are written for an audience that is knowledgable about statistics and operational research and has experience with other statistical computing programs. In contrast, this course intends to make R accessible to you, especially those who are new to both statistical concepts and statistical computing. 3.4 Hot and cold running resources Much is available in books, e-books, and online for free. This is an extensive online community that links expert and novice modelers globally. The standard start-up is at CRAN http://cran.r-project.org/manuals.html. A script in the appendix can be dropped into a workspace and played with easily. You can easily skip the rest of this expose on isntalling R, the integrated development environment RStudio, and some generous quips about computing, by simply goingn to the excellent on-line (and free) resource by James D. Long and Paul Teetor. 2019. R Cookbook 2nd Edition. O’Reilley: Sebastopol, CA. A version of this resource is with extensive R and RStudio installation instructions. Other resources include: Julian Faraway’s https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf complete course on regression where you can imbibe deeply of the many ways to use R in statistics. Along econometrics lines is Grant Farnsworth’s https://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf. Winston Chang’s http://www.cookbook-r.com/ and Hadley Wickham’s example at http://ggplot2.org/ are terrific online graphics resources. Psychologist Danielle Navarro’s Libretext https://learningstatisticswithr.com is a great companion to any statistical reasoning we might perform. 3.5 Installing R R needs to be installed on your computer. Anyway, R is freely distributed online, and you can download it from the R homepage, which is: http://cran.r-project.org/ At the top of the page – under the heading “Download and Install R” – you’ll see separate links for Windows users, Mac users, and Linux users. If you follow the relevant link, you’ll see that the online instructions are pretty self-explanatory. As of this writing, the current version of R is 4.0.2 “Taking Off Again” – and each new version will have even more creative names. Here is what you would see at the console. R version 4.0.2 (2020-06-22) -- &quot;Taking Off Again&quot; Copyright (C) 2020 The R Foundation for Statistical Computing Platform: x86_64-w64-mingw32/x64 (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type &#39;license()&#39; or &#39;licence()&#39; for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type &#39;contributors()&#39; for more information and &#39;citation()&#39; on how to cite R or R packages in publications. Type &#39;demo()&#39; for some demos, &#39;help()&#39; for on-line help, or &#39;help.start()&#39; for an HTML browser interface to help. Type &#39;q()&#39; to quit R. R updates every six months or so, but don’t worry, the now current version will suffice. 3.5.1 Installing R on a Windows computer The CRAN homepage changes from time to time, and it’s very old-school web design, but you can usually find what you are look for. In general you’ll find a link at the top of the page with the text “Download R for Windows”. If you click on that, it will take you to a page that offers you a few options. Again, at the very top of the page you’ll be told to click on a link that says to click here if you’re installing R for the first time. That’s probably what you want. This will take you to a page that has a prominent link at the top called “Download R 4.0.2 for Windows”. That’s the one you want. Click on that and your browser should start downloading a file called R-4.0.2-win.exe, or whatever the equivalent version number is by the time you read this. The file for version 4.0.2 is about 84MB in size, so it may take some time depending on how fast your connection is. Once you’ve downloaded the file, double click to install it. As with any software you download online, Windows will ask you some questions about whether you trust the file and if it can alter your system. Say yes. After you click through those, it’ll ask you where you want to install it, and what components you want to install. Your PC should be a 64 bit machine to run the routines for the rest of this book. The default values should be fine for most people, so again, just click through. Once all that is done, you should have R installed on your system. You can access it from the Start menu, or from the desktop if you asked it to add a shortcut there. While it may be some fun to open up R through the desktop shortcut, I suggest is that instead of doing that you should now install RStudio (see Section 3.5.4 for instructions). One other thing (always one more thing!): to run the tools in this book, you must install RTools40 for Windows based systems. This will allow you to use the Rcpp C++ compiler that is under the hood of the rstan, rethinking, brms, and tidybayes packages. For more details visit https://rstan.org. 3.5.2 Installing R on a Mac When you click on the Mac OS X link, you should find yourself on a page with the title “R for Mac OS X”. The vast majority of Mac users will have a fairly recent version of the operating system: as long as you run macOS 10.13 (High Sierra), then you’ll be fine.[^old] [^old]; If you’re running an older version of the Mac OS, then you need to follow the link to the “old” page (http://cran.r-project.org/bin/macosx/old/). You should be able to find the installer file that you need at the bottom of the page.] There’s a fairly prominent link on the page called “R-4.0.2.pkg”, which is the one you want. Click on that link and you’ll start downloading the installer file, which is R-4.0.2.pkg. It’s about 84MB in size. Once you’ve downloaded R-3.0.2.pkg, all you need to do is open it by double clicking on the package file. The installation should go smoothly from there: just follow all the instructions just like you usually do when you install something. Once it’s finished, you’ll find a file called R.app in the Applications folder. You can now open up R in the usual way14 if you want to, but what I’m going to suggest is that instead of doing that you should now install RStudio (see Section 3.5.4 for instructions). 3.5.3 Installing R on a Linux computer If you run a Linux box, regardless of what distribution, then you should find the instructions on the website easy enough. You can compile R from source yourself if you want, or install it through your package management system, which will probably have R in it. Alternatively, the CRAN site has precompiled binaries for Debian, Red Hat, Suse and Ubuntu and has separate instructions for each. Once you’ve got R installed, you can run it from the command line just by typing R. However, if you’re feeling envious of Windows and Mac users for their fancy GUIs, you can download RStudio too (see Section 3.5.4 for instructions). 3.5.4 Downloading and installing RStudio When you install R initially, it comes with one application that lets you do run R natively in an R GUI: it’s the R.exe application on a Windows machine, and the R.app application on a Mac. There are many integrated development environments (IDE) that will look for R and display the R console, while also allowing you to manage files, projects, look at your coding history, and review a variety of other objects. Jupyter is one such system that runs Julia, Python, and R interoperatively. The one I use exclusively, mainly for its one-stop-shop philosophy of data management, analysis cycles, and production of results in reports, books (like this one), and presentations, is RStudio. You can download the free personal version of RStudio here: http://www.RStudio.org/ When you visit the RStudio website, you’ll see a new school user interface, much easier to navigate and simpler than the CRAN website,15 Just click the Download button and follow the directions to the desktop version for your system. After it’s finished installing, you start R by opening RStudio. You don’t need to open R.app or R.exe in order to access R. RStudio will take care of that for you. In this screenshot you can see four panels. One of them is the R console. Rstudio Screen Shot The midnight blue background helps my eyesight. Your very first Rstudio job is to set up a new Project. This will create a *.Rproj file in a working directory. It is in that working directory that you will perform all of your computing. I use one for each week of instruction, for each paper or presentation I am writing, for this book too. Your programming life will thank you for this habit. RStudio has extensive guidance on everything from authoring slides to numerous cheatsheets that will help us navigate packages. The only shortcoming with RStudio is that it’s a perpetual work in progress: they keep improving it! Having said that I have found that updating RStudio is easy, and does not interfere with my workflows, file directories, R installation, packages, and so forth. 3.6 First day at school Always with some fear and trepidation we enter a new phase of learning. Let’s dive right into the deep end of this pool. 3.6.1 Install R Markdown Click on RStudio in your tray or start up menu. Be sure you are connected to the Internet. A console panel will appear. At the console prompt &gt; type install.packages(&quot;rmarkdown&quot;) This action will install the rmarkdown package. This package will enable you to construct documentation for your work as well as actually run the code you build. If you have several packages to attach to your workspace you can use this function to check if the packages are installed, attach installed packages, and install packages not yet installed. You can copy and paste the is_installed() function (yes, your first function) and the pkg vector into the console. Then we call the function with the pkg argument. is_installed &lt;- function(x) { for (i in x) { # require will return TRUE (1) # &#39;invisibly&#39; if it was able to load # package if (!require(i, character.only = TRUE)) { # If, for any reason, the package was # not able to be loaded then # re-install and expect a RStudio # message install.packages(i, dependencies = TRUE) # Attach the package after installing require(i, character.only = TRUE) } } } # for example here is a vector of # concatenated character strings # assigned to the obj `pkg` pkg &lt;- c(&quot;rmarkdown&quot;, &quot;shiny&quot;, &quot;psych&quot;, &quot;knitr&quot;, &quot;tidyverse&quot;, &quot;ggthemes&quot;, &quot;plotly&quot;, &quot;moments&quot;, &quot;flexdashboard&quot;, &quot;GGally&quot;) # use `pkg` as the &#39;argument&#39; of the # function `is_installed()` is_installed(pkg) Voila and we have packages in our library. We will add more packages as we go along. But these will suffice for now. This extremely helpful web page, http://rmarkdown.rstudio.com/gallery.html, is a portal to several examples of R Markdown source files that can be loaded into RStudio, modified, and used with other content for your own work. 3.7 Install LaTex Even though matheatics is kept to a low roar in this book, we can profitably use a comprehensive text rendering system for our documents. R Markdown uses a text rendering system called LaTeX to render text, including mathematical and graphical content. We can install thetinytex, or MikTeX document rendering system for Windows or MacTeX document rendering system for Mac OS X. This book is rendered with tinytex. For tinytexjust follow Yihui’s instructions: Installing and maintaining TinyTeX is easy for R users, since the R package tinytex has provided wrapper functions (N.B. the lowercase and bold tinytex means the R package, and the camel-case TinyTeX means the LaTeX distribution). You can use tinytex to install TinyTeX: install.packages(&#39;tinytex&#39;) tinytex::install_tinytex() # to uninstall TinyTeX, run tinytex::uninstall_tinytex() Yes, that simple. For Windows, navigate to the https://miktex.org/download page and go to the 64- or 32- bit installer. Click on the appropriate Download button and follow the directions. Be very sure you select the COMPLETE installation. Frequently Asked Questions (FAQ) can be found at https://docs.miktex.org/faq/. If you have RStudio already running, you will have to restart your session. For MAC OS X, navigate to the http://www.tug.org/mactex/ page and download the MacTeX system and follow the directions. This distribution requires Mac OS 10.5 Leopard or higher and runs on Intel or PowerPC processors. Be very sure you select the FULL installation. Frequently Asked Questions (FAQ) can be found at https://docs.miktex.org/faq/. If you have RStudio already running, you will have to restart your session. FAQ can be found at http://www.tug.org/mactex/faq/index.html. 3.7.1 Our first file Open RStudio and see something like this screenshot… Rstudio screenshot You can modify the position and content of the four panes by selecting View &gt; Panes &gt; Pane Options. If you haven’t already, definitely install.packages(\"rmarkdown\"). Then in the console again enter library(rmarkdown). Under File &gt; New File &gt; Rmarkdown a dialog box invites you to open document, presentation, Shiny, and other files. Upon choosing documents you may open up a new file. Under File &gt; Save As save the untitle file in an appropriate directory. The R Markdown file extension Rmd will appear in the file name in your directory. When creating a new Rmarkdown file, RStudio deposits a template that shows you how to use the markdown approach. You can generate a document by clicking on knit in the icon ribbon attached to the file name tab in the script pane. If you do not see knit, then you might need to install and load the knitr package with the following statements in the R console. You might need also to restart your RStudio session. install.packages(&quot;knitr&quot;) library(knitr) # also can use `library()` The Rmd file contains three types of content: An (optional) YAML header surrounded by --- on the top and the bottom of YAML statements. YAML is “Yet Another Markdown (or up) Language”. Here is an example from this document: --- title: &quot;Setting Up R for Analytics&quot; author: &quot;Bill Foote&quot; date: &quot;November 11, 2016&quot; output: pdf_document --- Chunks of R code surrounded by ``` (find this key usually as the lowercase of the ~ symbol). Text mixed with text formatting like # heading and _italics_ and mathematical formulae like $z = \\frac{(\\bar x-\\mu_0)}{s/\\sqrt{n}}$ which will render \\[z = \\frac{(\\bar x-\\mu_0)}{s/\\sqrt{n}}\\]. When you open an .Rmd file, RStudio provides an interface where code, code output, and text documentation are interleaved. You can run each code chunk by clicking the Run icon (it looks like a play button at the top of the chunk), or by pressing Cmd/Ctrl + Shift + Enter. RStudio executes the code and displays the results in the console with the code. You can write mathematical formulae in an R Markdown document as well. For example, here is a formula for net present value. $$ NPV = \\sum_{t=0}^{T} \\frac{NCF_t}{(1+WACC)^t} $$ This script will render \\[ NPV = \\sum_{t=0}^{T} \\frac{NCF_t}{(1+WACC)^t} \\] Here are examples of common in file text formatting in R Markdown. Text formatting ------------------------------------------------------------ *italic* or _italic_ **bold** __bold__ _**bold and italic**_ `code` superscript^2 and subscript_2 Headings ------------------------------------------------------------ # 1st Level Header ## 2nd Level Header ### 3rd Level Header Lists ------------------------------------------------------------ * Bulleted list item 1 * Item 2 * Item 2a * Item 2b 1. Numbered list item 1 1. Item 2. The numbers are incremented automatically in the output. Links and images ------------------------------------------------------------ &lt;http://example.com&gt; [linked phrase](http://example.com) ![optional caption text](path/to/img.jpg) Tables ------------------------------------------------------------ First Header | Second Header ------------- | ------------- Content Cell | Content Cell Content Cell | Content Cell Math ------------------------------------------------------------ $\\frac{\\mu}{\\sigma^2}$ \\[\\frac{\\mu}{\\sigma^2}] $$ \\frac{\\mu}{\\sigma^2} $$ More information will be provided on rmarkdown documentation throughout the course and can be found here with CRAN documentation and here at RStudio as well. 3.8 Some fun with Stan We will use the rethinking and brms packages for our work in this book. They depend on the rstan package which is an interface to the C++ STAN library for probabilistic computation. So our first step is navigate to the STAN site and to the rstan github site to install the rstan interface to Stan. Follow the directions exactly and in order. Failure to do so will cause much heart-ache, ulcers and other physical, mental, and emotional disorders! Because Stan models and any package that depends on Stan are compiled in C++ it is critically important to check the C++ tool chain in your system. If you have a windows OS then you must install Rtools40 as mentioned above in the R installation notes. The Rcpp package runs the interface to the C++ compiler. Follow the getting started directions in the order presented and closely. Remove the -march=native flag in the Documents/.R/Makevars.win file if on a Windows OS. If you get a Error: 'makevars_user' is not an exported object from 'namespace:withr' or similar error then remove the withr and rstan packages and re-install withr first, then rstan. In Rstudio you can check the syntax of the YOURMODELFILE.stan using rstan:::rstudio_stanc(\"MODELFILE.stan\") to ensure that all the code is correct. Also you must put a blank line after the last line of code. When running stan() we might see an error like '-E' not found – ignore it as it seems to come from the use of g++ compiler. Speaking of the C++ compiler, on windows especially, install Rtools40. Be sure to look up where the g++.exe file is and modify the .Renviron wherever your R system is located, usually in the Documents file. STAN runs through rstan on Rcpp. We need to be sure that Rcpp works properly. Test this idea with the simple program library(Rcpp) evalCpp(&quot;1+5&quot;) ## [1] 6 If this simple program does not work, check PATH and BINPREF and your Rtools40 implementation. hen following the instructions on the rethinking site and on the brms site. DOn’t forget the dolbe quotes below. The :: operator allows us to use a function from a package, in this case install_github from the devtools package. install.packages(c(&quot;devtools&quot;, &quot;coda&quot;, &quot;mvtnorm&quot;, &quot;devtools&quot;, &quot;loo&quot;, &quot;dagitty&quot;)) devtools::install_github(&quot;rmcelreath/rethinking&quot;) devtools::install_github(&quot;paul-buerkner/brms&quot;) We can copy this code into Rstudio to test whether any of this is working. library(rethinking) # model f &lt;- alist(y ~ dnorm(mu, sigma), mu ~ dnorm(0, 10), sigma ~ dexp(1)) # quadratic approximation fit &lt;- quap(f, data = list(y = c(-1, 1)), start = list(mu = 0, sigma = 1)) summary(fit) ## mean sd 5.5% 94.5% ## mu 0.00 0.59 -0.95 0.95 ## sigma 0.84 0.33 0.31 1.36 # precis( fit ) yields the same # output as summary() McElreath loves his 89% probability intervals! 3.9 Github Get a github account to store your work.. This is but a first step in the transparency needed to support solid data analytics solutions as effectively software development. You can also track your work, collaborate, manage versions and issues. This book is served from github pages linked to a repository. ## jaRgon (Mostly directly copied from Patrick Burns, and annotated a bit, for educational use only.) atomic vector An object that contains only one form of data. The atomic modes are: logical, numeric, complex and character. attach The act of adding an item to the search list. You usually attach a package with the require function, you attach saved files and objects with the attach function. data frame A rectangular data object where each column may be a different type of data. Conceptually a generalization of a matrix, but implemented entirely differently. This is a tibble() in the tidyverse. factor A data object that represents categorical data. It is possible (and often unfortunate) to confuse a factor with a character vector. global environment The first location on the search list, and the place where objects that you create reside. See search list. list A type of object with possibly multiple components where each component may be an arbitrary object, including a list. Each object can can different dimensions and data types. matrix A rectangular data object where all cells have the same data type. Conceptually a specialization of a data frame, but implemented entirely differently. This object has rows and columns. package A collection of R objects in a special format that includes help files, functions, examples, data, and source code. Most packages primarily or exclusively contain functions, but some packages exclusively contain datasets. Packages are attached to workspaces using the library() function. We use the require() function only to test if we have a package since this function returns a logical value (TRUE or FALSE). search list The collection of locations that R searches for objects when it is evaluating a command. The attribute simple is relative. For example, this Excel formula with nested if statements is far from simple to understand, document, and implement: `=if(x3 = 1, 2, if(y2=0, “42”&amp;cell(3, 2), if(left(a\\(4\\), find(b\\(4\\), “=42”))))).↩ Tip for advanced Mac users. You can run R from the terminal if you want to. The command is just “R”. It behaves like the normal desktop version, except that help documentation behaves like a “man” page instead of opening in a new window.↩ This is probably no coincidence: the people who design and distribute the core R language itself are focused on technical stuff. And sometimes they almost seem to forget that there’s an actual human user at the end. The people who design and distribute RStudio are focused on the user. Their goal is to make R as available, usable, auditable as possible.↩ "],
["tickling-the-ivories.html", "Chapter 4 Tickling the Ivories 4.1 Start to tickle 4.2 Try this exercise 4.3 Building Some Character 4.4 The plot thickens 4.5 Arrays and You 4.6 More Array Work 4.7 Summary 4.8 Further Reading 4.9 Practice Sets 4.10 Project: Captive Financing", " Chapter 4 Tickling the Ivories In this chapter we will use the R console in RStudio to run through the basic syntax of R. But that’s not all: we will also use R’s innate ability to implement linear algebra to build the basic statistics of ordinary least squares (OLS) regression. 4.1 Start to tickle Or if you paint and draw, the 2-minute pose will warm you up. In the RStudio console panel (in the NW pane of my IDE) play with these by typing these statements at the &gt; symbol: 1 + (1:5) ## [1] 2 3 4 5 6 This will produce a vector from 2 to 6. We can use alt- (hold alt and hyphen keys down simultaneously) to produce &lt;-, and assign data to an new object. This is a from R’s predecessor James Chamber’s S (ATT Bell Labs) that was ported from the single keystroke \\(\\leftarrow\\) in Ken Iverson’s APL (IBM), where it is reserved as a binary logical operator. We can now also use = to assign variables in R. But, also a holdover from APL, we will continue to use = only for assignments within functions. [Glad we got that over!] Now let’s try these expressions. x &lt;- 1 + (1:5) sum(x) ## [1] 20 prod(x) ## [1] 720 These actions assign the results of a calculation to a variable x and then sum and multiply the elements. x is stored in the active workspace. You can verify that by typing ls() in the console to list the objects in the workspace. Type in these statements as well. ls() ## [1] &quot;A.error&quot; &quot;A_col&quot; &quot;A_inner&quot; ## [4] &quot;A_inner_invert&quot; &quot;A_max&quot; &quot;A_min&quot; ## [7] &quot;A_result&quot; &quot;A_row&quot; &quot;A_sym&quot; ## [10] &quot;all&quot; &quot;argument&quot; &quot;beta_hat&quot; ## [13] &quot;C&quot; &quot;caption&quot; &quot;cashflow&quot; ## [16] &quot;check&quot; &quot;conclusion&quot; &quot;conjectures&quot; ## [19] &quot;d&quot; &quot;data_xy&quot; &quot;draw&quot; ## [22] &quot;e&quot; &quot;e_se&quot; &quot;e_sse&quot; ## [25] &quot;f&quot; &quot;fill&quot; &quot;fit&quot; ## [28] &quot;fit_0&quot; &quot;fit_1&quot; &quot;grid&quot; ## [31] &quot;hands&quot; &quot;hypotheses&quot; &quot;indicator&quot; ## [34] &quot;k&quot; &quot;labels&quot; &quot;lik&quot; ## [37] &quot;lines_1&quot; &quot;lines_2&quot; &quot;logic_df&quot; ## [40] &quot;major&quot; &quot;minor&quot; &quot;move_over&quot; ## [43] &quot;n&quot; &quot;n.sim&quot; &quot;n_blue&quot; ## [46] &quot;n_sim&quot; &quot;n_white&quot; &quot;position&quot; ## [49] &quot;posterior_hands&quot; &quot;posterior_hands_raw&quot; &quot;posterior_shovels&quot; ## [52] &quot;posterior_shovels_raw&quot; &quot;postSample&quot; &quot;priors&quot; ## [55] &quot;prob&quot; &quot;prod&quot; &quot;pv.1&quot; ## [58] &quot;pv.machine&quot; &quot;pv.salvage&quot; &quot;r&quot; ## [61] &quot;R&quot; &quot;randomSample&quot; &quot;rates&quot; ## [64] &quot;salvage&quot; &quot;schools_dat&quot; &quot;shovels&quot; ## [67] &quot;sum_column&quot; &quot;sum_row&quot; &quot;t&quot; ## [70] &quot;table_posteriors&quot; &quot;table_priors&quot; &quot;trueMu&quot; ## [73] &quot;trueSig&quot; &quot;truth&quot; &quot;x&quot; ## [76] &quot;X&quot; &quot;x.char&quot; &quot;x_1&quot; ## [79] &quot;x_2&quot; &quot;x1&quot; &quot;XTX_inverse&quot; ## [82] &quot;xy_df&quot; &quot;xy_summary&quot; &quot;xy_tbl&quot; ## [85] &quot;y&quot; &quot;z&quot; length(x) ## [1] 5 x[1:length(x)] ## [1] 2 3 4 5 6 x[6:8] ## [1] NA NA NA x[6:8] &lt;- 7:9 x/0 ## [1] Inf Inf Inf Inf Inf Inf Inf Inf x has length of 5 and we use that to index all of the current elements of x. Trying to access elements 6 to 8 produces na because they do not exist yet. Appending 7 to 9 will fill the spaces. Dividing by 0 produces inf. (x1 &lt;- x - 2) ## [1] 0 1 2 3 4 5 6 7 x1 ## [1] 0 1 2 3 4 5 6 7 x/x1 ## [1] Inf 3.0 2.0 1.7 1.5 1.4 1.3 1.3 Putting parentheses around an expression is the same as printing out the result of the expression. Element-wise division (multiplication, addition, subtraction) produces inf as the first element. 4.2 Try this exercise Suppose we have a gargleblaster machine that produces free cash flows of $10 million each year for 8 years. The machine will be scrapped and currently you believe you can get $5 million at the end of year 8 as salvage value. The forward curve of interest rates for the next 1 to 8 years is 0.06, 0.07, 0.05, 0.09, 0.09, 0.08, 0.08, 0.08. What is the value of $1 received at the end of each of the next 8 years? Use this script to begin the modeling process. Describe each calculation. rates &lt;- c(0.06, 0.07, 0.05, 0.09, 0.09, 0.08, 0.08, 0.08) t &lt;- seq(1, 8) (pv.1 &lt;- sum(1/(1 + rates)^t)) What is the present value of salvage? Salvage would be at element 8 of an 8-element cash flow vector, and thus would use the eighth forward rate, rate[8], and t would be 8 as well. Eliminate the sum in the above script. Make a variable called salvage and assign salvage value to this variable. Use this variable in place of the 1 in the above script for pv.1. Call the new present value pv.salvage. What is the present value of the gargleblaster machine? Type in these statements. The rep function makes an 8 element cash flow vector. We change the value of the 8th element of the cash flow vector to include salvage. Now use the pv.1 statement above and substitute cashflow for 1. You will have your result. cashflow &lt;- rep(10, 8) cashflow[8] &lt;- cashflow[8] + salvage Some results follow. The present value of $1 is The present value of a $1 is this mathemetical formula. \\[ PV = \\sum_{t=1}^{8}\\frac{1}{(1+r)^t} \\] This mathematical expression can be translated into R this way rates &lt;- c(0.06, 0.07, 0.05, 0.09, 0.09, 0.08, 0.08, 0.08) t &lt;- seq(1, 8) (1/(1 + rates)^t) ## [1] 0.94 0.87 0.86 0.71 0.65 0.63 0.58 0.54 (pv.1 &lt;- sum(1/(1 + rates)^t)) ## [1] 5.8 We define rates as a vector using the c() concatenation function. We then define a sequence of 8 time indices t starting with 1. The present value of a $1 is sum of the vector element-by-element calculation of the date by date discounts \\(1/(1+r)^t\\). The present value of salvage is the discounted salvage that is expected to occur at, and in this illustration only at, year 8. \\[ PV_{salvage} = \\frac{salvage}{(1+r)^8} \\] Translated into R we have salvage &lt;- 5 (pv.salvage &lt;- salvage/(1 + rates[8])^8) ## [1] 2.7 The present value of the gargleblaster machine is the present value of cashflows from operations from year 1 to year 8 plus the present value of salvage received in year 8. Salvage by definition is realized at the of the life of the operational cashflows upon disposition of the asset, here at year 8. \\[ PV_{total} = \\sum_{t=1}^{8}\\frac{cashflow_t}{(1+r)^t} + \\frac{salvage}{(1+r)^8} \\] This expression translates into R this way: cashflow &lt;- rep(10, 8) cashflow[8] &lt;- cashflow[8] + salvage (pv.machine &lt;- sum(cashflow/(1 + rates)^t)) ## [1] 61 The rep or “repeat” function creates cash flows of $10 for each of 8 years. We adjust the year 8 cash flow to reflect salvage so that \\(cashflow_8 = 10 + salvage\\). The [8] indexes the eighth element of the cashflow vector. 4.3 Building Some Character Let’s type these expressions into the console at the &gt; prompt: x[length(x) + 1] &lt;- &quot;end&quot; x[length(x) + 1] &lt;- &quot;end&quot; x.char &lt;- x[-length(x)] x &lt;- as.numeric(x.char[-length(x.char)]) str(x) ## num [1:8] 2 3 4 5 6 7 8 9 We have appended the string “end” to the end of x, twice. We use the - negative operator to eliminate it. By inserting a string of characters into a numeric vector we have forced R to transform all numerical values to characters. To keep things straight we called the character version x.char. In the end we convert x.char back to numbers that we check with the str(ucture) function. We will use this procedure to build data tables (we will call these “data frames”) when comparing distributions of variables such as stock returns. Here’s a useful set of statements for coding and classifying variables. Type these statements into the console. set.seed(1016) n.sim &lt;- 10 x &lt;- rnorm(n.sim) y &lt;- x/(rchisq(x^2, df = 3))^0.5 We did a lot of R here. First, we set a random seed to reproduce the same results every time we run this simulaton. Then, we store the number of simulations in n.sim and produced two new variables with normal and a weirder looking distribution (a Student’s t distribution?). Invoking help will display help with distributions in the console pane of the RStudio IDE. Now let’s try to display some of this interesting, and if surprising, information. The next code block will set up some presentation layer data for a plot. z &lt;- c(x, y) indicator &lt;- rep(c(&quot;normal&quot;, &quot;abnormal&quot;), each = length(x)) xy_df &lt;- data.frame(Variates = z, Distributions = indicator) We concatenate the two variables into a new variable z. We built into the variable indicator the classifier to indicate which is x and which is y. But let’s visualize what we want. (Paint in words here.) We want a column the first n.sim elements of which are x and the second are y. We then want a column the first n.sim elements of which are indicated by the character string “normal”, and the second n.sim elements by “abnormal”. The rep function replicates the concatenation of “normal” and “abnormal” 10 times (the length(x)). The each feature concatenates 10 replications of “normal” to 10 replications of “abnormal”. We concatenate the variates into xy with the c() function. Data frames are just column and row tables. Enter str(xy_df) to see what the structure of the xy_df data frame contains. In later work we will use a streamlined version of the data frame called a tibble in the tidyverse ecosystem. We can see the first 5 components of the data frame components using the $ subsetting notation as below. str(xy_df) ## &#39;data.frame&#39;: 20 obs. of 2 variables: ## $ Variates : num 0.777 1.373 1.303 0.148 -1.825 ... ## $ Distributions: chr &quot;normal&quot; &quot;normal&quot; &quot;normal&quot; &quot;normal&quot; ... head(xy_df$Variates, n = 5) ## [1] 0.78 1.37 1.30 0.15 -1.83 head(xy_df$Distributions, n = 5) ## [1] &quot;normal&quot; &quot;normal&quot; &quot;normal&quot; &quot;normal&quot; &quot;normal&quot; The str call returns the two vectors inside of xy. One is numeric and the other is a “factor” with two levels. R and many of the routines in R will interpret these as zeros and ones in developing indicator and dummy variables for regressions and filtering. The head() (and there is a tail() too) function displays as many components as you wish to see starting with row 1. 4.4 The plot thickens We will want to see our handiwork, so load the ggplot2 library using install.packages(\"ggplot2\").16 This plotting package requires data frames. A “data frame” simply put is a list of vectors and arrays with names. An example of a data frame in Excel is just the worksheet. There are columns with names in the first row, followed by several rows of data in each column. If you were to install the tidyverse package and load the package with library(tidyverse) you could use tibble() instead of the bas R data.frame(). Here we have defined a data frame xy_df. All of the x and y variates are put into one part of the frame, and the distribution indicator into another. For all of this to work in a plot the two arrays must be of the same length. Thus we use the common n.sim and length(x) to insure this when we computed the series. We always examine the data, here using the head and tail functions. Type help(ggplot) into the console for details. The ggplot2 graphics package embodies Hadley Wickham’s “grammar of graphics” we can review at http://ggplot2.org. Hadley Wickham has a very useful presentation with numerous examples at http://ggplot2.org/resources/2007-past-present-future.pdf. As mentioned above, the package uses data frames to process graphics. A lot of packages other than ggplot2, including the base stats package, require data frames. We load the library first. The next statement sets up the blank but all too ready canvas (it will be empty!) on which a density plot can be rendered. library(ggplot2) ggplot(xy_df, aes(x = Variates, fill = Distributions)) The data frame name xy_df is first followed by the aesthetics mapping of data. The next statement inserts a geometrical element, here a density curve, which has a transparency parameter aesthetic alpha. 4.4.1 Try this example Zoom in with xlim and lower x-axis and upper x-axis limits using the following statement: ggplot(xy_df, aes(x = Variates, fill = Distributions)) + geom_density(alpha = 0.3) + xlim(-1, 6) Now we are getting to extreme value statistics by visualizing the tail of this distribution. 4.5 Arrays and You Arrays have rows and columns and are akin to tables. All of Excel’s worksheets are organized into cells that are tables with columns and rows. Data frames are more akin to tables in data bases. Here are some simple matrix arrays and functions. We start by making a mistake: (A.error &lt;- matrix(1:11, ncol = 4)) ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 1 The matrix() function takes as input here the sequence of numbers from 1 to 11. It then tries to put these 11 elements into a 4 column array with 3 rows. It is missing a number as the error points out. To make a 4 column array out of 11 numbers it needs a twelth number to complete the third row. We then type in these statements (A_row &lt;- matrix(1:12, ncol = 4)) ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 (A_col &lt;- matrix(1:12, ncol = 4, byrow = FALSE)) ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 In A we take 12 integers in a row and specify they be organized into 4 columns, and in R this is by row. In the next statement we see that A_col and column binding cbind() are equivalent. (R &lt;- rbind(1:4, 5:8, 9:12)) # Concatenate rows ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 (C &lt;- cbind(1:3, 4:6, 7:9, 10:12)) # concatenate columns ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 A_col == C ## [,1] [,2] [,3] [,4] ## [1,] TRUE TRUE TRUE TRUE ## [2,] TRUE TRUE TRUE TRUE ## [3,] TRUE TRUE TRUE TRUE Using the outer product allows us to operate on matrix elements, first picking the minimum, then the maximum of each row. The pmin and pmax compare rows element by element. If you used min and max you would get the minimum and maximum of the whole matrix. (A_min &lt;- outer(3:6/4, 3:6/4, FUN = pmin)) # ## [,1] [,2] [,3] [,4] ## [1,] 0.75 0.75 0.75 0.75 ## [2,] 0.75 1.00 1.00 1.00 ## [3,] 0.75 1.00 1.25 1.25 ## [4,] 0.75 1.00 1.25 1.50 (A_max &lt;- outer(3:6/4, 3:6/4, FUN = pmax)) # ## [,1] [,2] [,3] [,4] ## [1,] 0.75 1.0 1.2 1.5 ## [2,] 1.00 1.0 1.2 1.5 ## [3,] 1.25 1.2 1.2 1.5 ## [4,] 1.50 1.5 1.5 1.5 We build a symmetrical matrix and replace the diagonal with 1. A_sym looks like a correlation matrix. Here all we were doing is playing with shaping data. (A_sym &lt;- A_max - A_min - 0.5) ## [,1] [,2] [,3] [,4] ## [1,] -0.50 -0.25 0.00 0.25 ## [2,] -0.25 -0.50 -0.25 0.00 ## [3,] 0.00 -0.25 -0.50 -0.25 ## [4,] 0.25 0.00 -0.25 -0.50 diag(A_sym) &lt;- 1 A_sym ## [,1] [,2] [,3] [,4] ## [1,] 1.00 -0.25 0.00 0.25 ## [2,] -0.25 1.00 -0.25 0.00 ## [3,] 0.00 -0.25 1.00 -0.25 ## [4,] 0.25 0.00 -0.25 1.00 4.5.1 Try this exercise The inner product %*% cross-multiplies successive elements of a row with the successive elements of a column. If there are two rows with 5 columns, there must be a matrix at least with 1 column that has 5 rows in it. Let’s run these statements. n_sim &lt;- 100 x_1 &lt;- rgamma(n_sim, 0.5, 0.2) x_2 &lt;- rlnorm(n_sim, 0.15, 0.25) hist(x_1) hist(x_2) X &lt;- cbind(x_1, x_2) rgamma allows us to generate n_sim versions of the gamma distribution with scale parameter 0.5 and shape parameter 0.2. rlnorm is a popular financial return distribution with mean 0.15 and standard deviation 0.25. We can call up ??distributions to get detailed information. Let’s plot the histograms of each simulated random variate using hist(). The cbind function binds into matrix columns the row arrays x_1 and x_2. These might be simulations of operational and financial losses. The X matrix could look like the design matrix for a regression. Let’s simulate a response vector, say equity, and call it y and look at its histogram. y &lt;- 1.5 * x_1 + 0.8 * x_2 + rnorm(n_sim, 4.2, 5.03) Now we have a frequentist statistical model for \\(y\\): \\[ y = X \\beta + \\varepsilon \\] where \\(y\\) is a 100 \\(\\times\\) 1 (rows \\(\\times\\) columns) vector, \\(X\\) is a 100 \\(\\times\\) 2 matrix, \\(\\beta\\) is a 2 \\(\\times\\) 1 vector, and \\(\\epsilon\\) is a 100 \\(\\times\\) 1 vector of disturbances (a.k.a., “errors”). Multiplying out the matrix term \\(X \\beta\\) we have \\[ y = \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon \\] where \\(y\\), \\(x_1\\), \\(x_2\\), and \\(\\varepsilon\\) are all vectors with 100 rows for simulated observations. If we look for \\(\\beta\\) to minimize the sum of squared \\(\\varepsilon\\) we would find that the solution is \\[ \\hat{\\beta} = (X^T X)^{-1} X^{T} y. \\] Where \\(\\hat{\\beta}\\) is read as “beta hat”. The result \\(y\\) with its hist() is hist(y) The rubber meets the road here as we compute \\(\\hat{\\beta}\\). X &lt;- cbind(x_1, x_2) XTX_inverse &lt;- solve(t(X) %*% X) (beta_hat &lt;- XTX_inverse %*% t(X) %*% y) ## [,1] ## x_1 1.6 ## x_2 4.0 The beta_hat coefficients are much different than our model for y. Why? Because of the innovation, error, disturbance term rnorm(n_sim, 1, 2) we added to the 1.5*x_1 + 0.8 * x_2 terms. Now for the estimated \\(\\varepsilon\\) where we use the matrix inner product %*%. We need to be sure to pre-multiply beta_hat with X! e &lt;- y - X %*% beta_hat hist(e) We see that the “residuals” are almost centered at 0. 4.5.2 More about residuals For no charge at all let’s calculate the sum of squared errors in matrix talk, along with the number of obervations n and degrees of freedom n - k, all to get the standard error of the regression e_se. Mathematically we are computing \\[ \\sigma_{\\varepsilon} = \\sqrt{\\sum_{i=1}^N \\frac{\\varepsilon_i^2}{n-k}} \\] (e_sse &lt;- t(e) %*% e) ## [,1] ## [1,] 3021 (n &lt;- dim(X)[1]) ## [1] 100 (k &lt;- nrow(beta_hat)) ## [1] 2 (e_se &lt;- (e_sse/(n - k))^0.5) ## [,1] ## [1,] 5.6 The statement dim(X)[1] returns the first of two dimensions of the matrix X. The R system has a built in OLS model called lm() (for linear model). Here is that model and a summary() of results. library(tidyverse) data_xy &lt;- tibble(x_1 = x_1, x_2 = x_2, y = y) fit_0 &lt;- lm(y ~ x_1 + x_2 - 1, data = data_xy) summary(fit_0) ## ## Call: ## lm(formula = y ~ x_1 + x_2 - 1, data = data_xy) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.015 -3.200 0.366 4.333 14.347 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## x_1 1.607 0.161 9.97 &lt; 0.0000000000000002 *** ## x_2 4.042 0.616 6.56 0.0000000026 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.5 on 98 degrees of freedom ## Multiple R-squared: 0.799, Adjusted R-squared: 0.795 ## F-statistic: 195 on 2 and 98 DF, p-value: &lt;0.0000000000000002 First we load the tidyverse package and make a tibble data set. Then we use the statistical model y ~ x_1 + x_2 - 1 The lm() function parses this formula into the dependent variable y, the two independent variables x_1 and x_2, and -1 for a zero intercept all referring to the components of the data set in the lm() function. We then can view the statistics of the regression using the summary() method associated with lm(). All results are in the object fit_0. Of course we must try our model with an intercept, the default in lm(). I usually put the 1 + into the formula as a reminder. fit_1 &lt;- lm(y ~ 1 + x_1 + x_2, data = data_xy) summary(fit_1) ## ## Call: ## lm(formula = y ~ 1 + x_1 + x_2, data = data_xy) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.768 -3.148 -0.674 3.700 13.362 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.313 2.221 2.39 0.019 * ## x_1 1.559 0.159 9.82 0.00000000000000033 *** ## x_2 -0.200 1.873 -0.11 0.915 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.4 on 97 degrees of freedom ## Multiple R-squared: 0.5, Adjusted R-squared: 0.489 ## F-statistic: 48.5 on 2 and 97 DF, p-value: 0.00000000000000257 We see a very different story where the coefficient of the x_2 variable changes sign. In future sagas we will bury \\(R^2\\). The lm() function is thew orkhorse of R regression in the frequentist statistical tradition of Joseph Venn and R.A. Fisher (1925).17 We also notice the layout of the writing of the function and its arguments. This tremendously aids comprehension as well as shooting trouble when running the functions. Finally, again for no charge at all, lets load library psych (use install.packages(\"psych\") as needed). We will use pairs.panels() for a pretty picture of our work in this try out. First column bind cbind() the y, X, and e arrays to create a data frame for pairs.panel(). library(psych) all &lt;- cbind(y, X, e) We then invoke the pairs.panels() function using the all array we just created. The result is a scatterplot matrix with histograms of each variate down the diagonal. The lower triangle of the matrix is populated with scatterplots. The upper triangle of the matrix has correlations depicted with increasing font sizes for higher correlations. pairs.panels(all) We will use this sort of tool again and again to explore the multivariate relationships among our data. Even better displays are available with the 4.6 More Array Work We show off some more array operations in the following statements. nrow(A_min) ## [1] 4 ncol(A_min) ## [1] 4 dim(A_min) ## [1] 4 4 We calculate the number of rows and columns first. We then see that these exactly correspond to the two element vector produced by dim. Next we enter these statements into the console. rowSums(A_min) ## [1] 3.0 3.8 4.2 4.5 colSums(A_min) ## [1] 3.0 3.8 4.2 4.5 apply(A_min, 1, sum) ## [1] 3.0 3.8 4.2 4.5 apply(A_min, 2, sum) ## [1] 3.0 3.8 4.2 4.5 We also calculate the sums of each row and each column. Alternatively we can use the apply function on the first dimension (rows) and then on the second dimension (columns) of the matrix. Some matrix multiplications follow below. (A_inner &lt;- A_sym %*% t(A_min[, 1:dim(A_min)[2]])) ## [,1] [,2] [,3] [,4] ## [1,] 0.75 0.75 0.81 0.88 ## [2,] 0.38 0.56 0.50 0.50 ## [3,] 0.38 0.50 0.69 0.62 ## [4,] 0.75 0.94 1.12 1.38 Starting from the inner circle of embedded parentheses we pull every row (the [,col] piece) for columns from the first to the second dimension of the dim() of A_min. We then transpose (row for column) the elements of A_min and cross left multiply in an inner product this transposed matrix with A_sym. We have already deployed very useful matrix operation, the inverse. The R function solve() provides the answer to the question: what two matrices, when multiplied by one another, produces the identity matrix? The identity matrix is a matrix of all ones down the diagonal and zeros elsewhere. (A_inner_invert &lt;- solve(A_inner)) ## [,1] [,2] [,3] [,4] ## [1,] 4.95238095238095255 -3.0 -1.1 -1.5 ## [2,] -2.28571428571428559 6.9 -2.3 0.0 ## [3,] 0.00000000000000013 -2.3 6.9 -2.3 ## [4,] -1.14285714285714302 -1.1 -3.4 3.4 Now we use our inverse with the original matrix we inverted. (A_result &lt;- A_inner %*% A_inner_invert) ## [,1] [,2] [,3] [,4] ## [1,] 0.99999999999999978 0.00000000000000000 0.00000000000000000 0.00000000000000000 ## [2,] 0.00000000000000022 1.00000000000000022 0.00000000000000000 0.00000000000000000 ## [3,] 0.00000000000000011 0.00000000000000011 1.00000000000000000 0.00000000000000044 ## [4,] 0.00000000000000022 -0.00000000000000044 -0.00000000000000089 1.00000000000000000 When we cross multiply A_inner with its inverse, we should, and do, get the identity matrix that is a matrix of ones in the diagonal and zeros in the off-diagonal elements. 4.7 Summary We covered very general data manipulation in R including arithmetical operations, vectors and matrices, their formation and operations, and data frames. We used data frames as inputs to plotting functions. We also built a matrix-based linear regression model and a present value calculator. This will be nearly the last time in this book we will employ frequentist statistical calculation. The next chapter will bring us to the brink of the statistical reasoning often called Bayesian. It is also nearly the last time we will seriously use the nouns and verbs of base R data wrangling. We will start next chapter to use modern data management nouns and verbs with the tidyverse package and ecosystem. 4.8 Further Reading This introductory chapter covers material from Teetor, chapters 1, 2, 5, 6. Present value, salvage, and other valuation topics can be found in Brealey et al. under present value in the index of any of several editions. 4.9 Practice Sets 4.9.1 Purpose, Process, Product These practice sets will repeat various R features in this chapter. Specifically we will practice defining vectors, matrices (arrays), and data frames and their use in present value, growth, future value calculations, We will build on this basic practice with the computation of ordinary lease squares coefficients and plots using ggplot2. We will summarize our findings in debrief documented with an R markdown file and output. 4.9.2 R Markdown set up Open a new R Markdown html document file and save it with file name MYName-FIN654-PS01 to your working directory. The Rmd file extension will automatically be appended to the file name. Create a new folder called data in this working directory and deposit the .csv file for practice set #2 to this directory. Modify the YAML header in the Rmd file to reflect the name of this practice set, your name, and date. Replace the R Markdown example in the new file with the following script. ## Practice set 1: present value (INSERT results here) ## Practice set 2: regression (Insert results here) Click knit in the Rstudio command bar to produce the html document. 4.9.3 Mutual Fund simulation 4.9.3.1 Problem We work for a mutual fund that is legally required to fair value the stock of unlisted companies it owns. Your fund is about to purchase shares of InUrCorner, a U.S. based company, that provides internet-of-things legal services. We sampled several companies with business plans similar to InUrCorner and find that the average weighted average cost of capital is 18%. InUrCorner sales is $80 million and projected to growth at 50% per year for the next 3 years and 15% per year thereafter. Cost of services provided as a percent of sales is currently 75% and projected to be flat for the foreseeable future. Depreciation is also constant at 5% of net fixed assets (gross fixed asset minus accumulated depreciation), as are taxes (all-in) at 35% of taxable profits. Discussions with InUrCorner management indicate that the company will need an increase in working capital at the rate of 15% each year and an increase in fixed assets at the rate of 10% of sales each year. Currently working capital is $10, net fixed assets is $90, and accumulated depreciation is $15. 4.9.3.2 Questions Let’s project sales, cost, increments to net fixed assets NFA, increments to working capital WC, depreciation, tax, and free cash flow FCF for the next 4 years. We will use a table to report the projection. Let’s use this code to build and display a table. # Form table of results table.names &lt;- c(&quot;Sales&quot;, &quot;Cost&quot;, &quot;Working Capital (incr.)&quot;, &quot;Net Fixed Assets (incr.)&quot;, &quot;Free Cash Flow&quot;) # Assign projection labels table.year &lt;- year # Assign projection years table.data &lt;- rbind(sales, cost, WC.incr, NFA.incr, FCF) # Layer projections rownames(table.data) &lt;- table.names # Replace rows with projection labels colnames(table.data) &lt;- table.year # Replace columns with projection years knitr::kable(table.data) # Display a readable table Modify the assumptions by +/- 10% and report the results. 4.9.4 Healthcare provider admission rates 4.9.4.1 Problem We work for a healthcare insurer and our management is interested in understanding the relationship between input admission and outpatient rates as drivers of expenses, payroll, and employment. We gathered a sample of 200 hospitals in a test market in this data set. x.data &lt;- read.csv(&quot;data/hospitals.csv&quot;) 4.9.4.2 Questions Build a table that explores this data set variable by variable and relationships among variables. Investigate the influence of admission and outpatient rates on expenses and payroll. First, form these arrays. Next, compute the regression coefficients. Finally, compute the regression statistics. Use this code to investigate further the relationship among predicted expenses and the drivers, admissions and outpatients. require(reshape2) require(ggplot2) actual &lt;- y predicted &lt;- X %*% beta.hat residual &lt;- actual - predicted results &lt;- data.frame(actual = actual, predicted = predicted, residual = residual) # Insert comment here min_xy &lt;- min(min(results$actual), min(results$predicted)) max_xy &lt;- max(max(results$actual), max(results$predicted)) # Insert comment here plot.melt &lt;- melt(results, id.vars = &quot;predicted&quot;) # Insert comment here plot.data &lt;- rbind(plot.melt, data.frame(predicted = c(min_xy, max_xy), variable = c(&quot;actual&quot;, &quot;actual&quot;), value = c(max_xy, min_xy))) # Insert comment here p &lt;- ggplot(plot, aes(x = predicted, y = value)) + geom_point(size = 2.5) + theme_bw() p &lt;- p + facet_wrap(~variable, scales = &quot;free&quot;) p 4.9.5 Practice Set Debrief List the R skills needed to complete these practice labs. What are the packages used to compute and graph results. Explain each of them. How well did the results begin to answer the business questions posed at the beginning of each practice lab? 4.10 Project: Captive Financing 4.10.1 Purpose This project will allow us to practice various R features using live data to support a decision regarding the provision of captive financing to customers at the beginning of this chapter. We will focus on translating regression statistics into R, plotting results, and interpreting ordinary least squares regression outcomes. 4.10.2 Problem As we researched how to provide captive financing and insurance for our customers we found that we needed to understand the relationships among lending rates and various terms and conditions of typical equipment financing contracts. We will focus on one question: What is the influence of terms and conditions on the lending rate of fully committed commercial loans with maturities greater than one year? 4.10.3 Data The data set commloan.csv contains data from the St. Louis Federal Reserve Bank’s FRED website we will use to get some high level insights. The quarterly data extends from the first quarter of 2003 to the second quarter of 2016 and aggregates a survey administered by the St. Louis Fed. There are several time series included. Each is by the time that pricing terms Were set and by commitment, with maturities more than 365 Days from a survey of all commercial banks. Here are the definitions. 4.10.4 Work Flow Prepare the data. Visit the FRED website. Include any information on the site to enhance the interpretation of results. Use read.csv to read the data into R. Be sure to set the project’s working directory where the data directory resides. Use na.omit() to clean the data. # setwd(&#39;C:/Users/Bill # Foote/bookdown/bookdown-demo-master&#39;) # the project directory x.data &lt;- read.csv(&quot;data/commloans.csv&quot;) x.data &lt;- na.omit(x.data) Assign the data to a variable called x.data. Examine the first and last 5 entries. Run a summary of the data set. What anomalies appear based on these procedures? Explore the data. Let’s plot the time series data using this code: require(ggplot2) require(reshape2) # Use melt() from reshape2 to build # data frame with data as id and # values of variables x.melted &lt;- melt(x.data[, c(1:4)], id = &quot;date&quot;) ggplot(data = x.melted, aes(x = date, y = value)) + geom_point() + facet_wrap(~variable, scales = &quot;free_x&quot;) Describe the data frame that melt() produces. Let’s load the psych library and produce a scatterplot matrix. Interpret this exploration. Analyze the data. Let’s regress rate on the rest of the variables in x.data. To do this we form a matrix of independent variables (predictor or explanatory variables) in the matrix X and a separate vector y for the dependent (response) variable rate. We recall that the 1 vector will produce a constant intercept in the regression model. y &lt;- as.vector(x.data[, &quot;rate&quot;]) X &lt;- as.matrix(cbind(1, x.data[, c(&quot;prepaypenalty&quot;, &quot;maturity&quot;, &quot;size&quot;, &quot;volume&quot;)])) head(y) head(X) Explain the code used to form y and X. Calculate the \\(\\hat{\\beta}\\) coefficients and interpret their meaning. Calculate actual and predicted rates and plot using this code. # Insert comment here require(reshape2) require(ggplot2) actual &lt;- y predicted &lt;- X %*% beta.hat residual &lt;- actual - predicted results &lt;- data.frame(actual = actual, predicted = predicted, residual = residual) # Insert comment here min_xy &lt;- min(min(results$actual), min(results$predicted)) max_xy &lt;- max(max(results$actual), max(results$predicted)) # Insert comment here plot.melt &lt;- melt(results, id.vars = &quot;predicted&quot;) # Insert comment here plot.data &lt;- rbind(plot.melt, data.frame(predicted = c(min_xy, max_xy), variable = c(&quot;actual&quot;, &quot;actual&quot;), value = c(max_xy, min_xy))) # Insert comment here p &lt;- ggplot(plot, aes(x = predicted, y = value)) + geom_point(size = 2.5) + theme_bw() p &lt;- p + facet_wrap(~variable, scales = &quot;free&quot;) p Insert explanatory comments into the code chunk to document the work flow for this plot. Interpret the graphs of actual and residual versus predicted values of rate. Calculate the standard error of the residuals, Interpret its meaning. References "],
["counting-on-tibble.html", "Chapter 5 Counting on tibble 5.1 Counting the ways 5.2 Having all of our marbles", " Chapter 5 Counting on tibble 5.1 Counting the ways Instead of the base R wrangling we began to use in the previous chapter, we’ll make extensive use of the many packages from the tidyverse for data wrangling and plotting. Much of the R programming is a direct fork (I guess as in sticking a fork into a plate of food and putting in our mouth) from Solomon Kurz’s recasting of McElreath (2020). library(tidyverse) The tidyverse-style syntax pipes %&gt;% nouns (data) into verbs (functions). In RStudio we can use the shift-ctrl-m key combination to make the pipe with space formatting before and after. A good practice is to use piping as if they are analytical layers and thus press enter after each %&gt;%to create a new row of row. The beauty of this approach is that we can follow the workflow as each piping literally creates new data from analytical step to analytical step in the workflow. Read chapter 5 of Grolemund and Wickham’s R for Data Science available online in Section 5.6.1. We will extensively use the ggplot2 (Wickham et al. 2020; Wickham 2016) system in the tidyverse. The gg stands for the grammar of graphics. In this framework, similar to the way Adobe Photoshop and other image processors work, we layer graphical elements onto a blank canvas. These elements start with a data frame, here a tibble. From the tibble axes on a canvas along with groupings of data using factors that are categorical variables in the tibble. We then put geometrical objects on top of this growing canvas. Objects include lines, points, text all with size, shape, and color, among other attributes. We can also make graphics interactive using the plotly package with tools such as brushing, zooming, and visually driven queries. The plotly site has much more information and many gallery examples of which we might avail ourselves. As interesting, and where plotly is going is it calls itself the front-end of ML [machine learning] and data science.. Tibbles (Müller and Wickham 2020) are the backbone of the management of the data we use in all of the functional workings of our model. First of all, a tibble is a data frame and has the two dimensions of any matrix or table, rows and columns. So, whenever we talk about data frames, we’re usually talking about tibbles. For more on the topic, check out R4SD, Chapter 10. It is often best to learn of tibbles by doing. Doing what? Why, it is piping data from a raw tibble to aggregations of the data, transformations of the raw data and aggregations. Here is a very simple example in the tidyverse that makes use of the dplyr package. Let’s make some toy data to play with. We generate 100 variates normally distributed with mean 10 and standard deviation 5. We then transform this series into another and display the first 10 rows. To reproduce results we set the random seed. library(tidyverse) set.seed(42) n_sim &lt;- 100 x &lt;- abs(rnorm(n_sim, 10, 5)) y &lt;- 3 + 0.5 * x xy_tbl &lt;- tibble(y = y, x = x) xy_tbl ## # A tibble: 100 x 2 ## y x ## &lt;dbl&gt; &lt;dbl&gt; ## 1 11.4 16.9 ## 2 6.59 7.18 ## 3 8.91 11.8 ## 4 9.58 13.2 ## 5 9.01 12.0 ## 6 7.73 9.47 ## 7 11.8 17.6 ## 8 7.76 9.53 ## 9 13.0 20.1 ## 10 7.84 9.69 ## # ... with 90 more rows With this tibble we can transform the variables by adding more through a pipe to the mutate() verb and assign the results to another tibble object. xy_tbl &lt;- xy_tbl %&gt;% mutate(log_y = log(y), log_x = log(x)) xy_tbl ## # A tibble: 100 x 4 ## y x log_y log_x ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 11.4 16.9 2.44 2.82 ## 2 6.59 7.18 1.89 1.97 ## 3 8.91 11.8 2.19 2.47 ## 4 9.58 13.2 2.26 2.58 ## 5 9.01 12.0 2.20 2.49 ## 6 7.73 9.47 2.05 2.25 ## 7 11.8 17.6 2.47 2.87 ## 8 7.76 9.53 2.05 2.25 ## 9 13.0 20.1 2.57 3.00 ## 10 7.84 9.69 2.06 2.27 ## # ... with 90 more rows Of course, in this code we overwrote the first version of the xy_tbl tibble. Next we aggregate this transformed tibble into a custom summary of the data. options(digits = 4, scipen = 99999) xy_summary &lt;- xy_tbl %&gt;% gather(key = &quot;variable&quot;, value = &quot;value&quot;) %&gt;% group_by(variable) %&gt;% summarize(min = min(value), pct_5 = quantile(value, 0.05), pct_50 = quantile(value, 0.5), pct_95 = quantile(value, 0.95), max = max(value)) xy_summary ## # A tibble: 4 x 6 ## variable min pct_5 pct_50 pct_95 max ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 log_x 0.0893 0.984 2.35 2.87 3.06 ## 2 log_y 1.27 1.47 2.11 2.47 2.62 ## 3 x 1.09 2.68 10.4 17.6 21.4 ## 4 y 3.55 4.34 8.22 11.8 13.7 Lots of things are happing to us here. options sets the rest of the calculations to 4 decimal places with sufficient penalties to avoid scientific notation. gather() puts the data into a long format, also known as a flat table. This allows us to group_by the subsequent aggregations in summarize() by variable, the data key. The transformed tibble has summarized()d each variable’s vital statistics in new tibble colums. The inverse of gather() is arrange() which awaits us in future work. This is a great start to exploring our data. Gosh, we could even write a helper function to wrap all of this into an easy to use replacement for the base R summary() function. Again this lovely activity awaits us in future work. 5.2 Having all of our marbles Let code the example in chapter 1. This includes red and blue voters. Let’s willingly suspend our disbelief and call them marbles in a bag. The bag is not transparent and those who draw a marble-voter from the bag are blind-folded. If they were draw a marble-voter from the bag they must return their find to the bag. If we’re willing to code the marbles as 0 = red 1 = blue, we can arrange the hypothetical possibilities of combinations of blue and red marble-voters in a tibble qua bag as follows. Again this is data. Attributes like color and shapes will indeed follow, but at the visualization layer of the architecture of this discussion and coding exercise. d &lt;- tibble(p1 = 0, p2 = rep(1:0, times = c(1, 3)), p3 = rep(1:0, times = c(2, 2)), p4 = rep(1:0, times = c(3, 1)), p5 = 1) d ## # A tibble: 4 x 5 ## p1 p2 p3 p4 p5 ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 1 1 1 1 ## 2 0 0 1 1 1 ## 3 0 0 0 1 1 ## 4 0 0 0 0 1 YATBL = yet another tibble, and they will not stop coming! In this one we use the rep() function to replicate first 1’s then down to 0’s once and then 3 times in p2. This creates a 4 row by 5 column array with upper 1’s and lower 0’s. We will visualize this arrange in a transformation of this tibble. First we use set_names() to replace the p1 to p5 column names. With mutate() we create row names of the 4 conjectures labelled 1 to 4. The pivot_longer() creates a flat grid of d &lt;- d %&gt;% set_names(1:5) %&gt;% mutate(x = 1:4) %&gt;% pivot_longer(-x, names_to = &quot;conjecture&quot;) %&gt;% mutate(value = value %&gt;% as.character()) d ## # A tibble: 20 x 3 ## x conjecture value ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 1 0 ## 2 1 2 1 ## 3 1 3 1 ## 4 1 4 1 ## 5 1 5 1 ## 6 2 1 0 ## 7 2 2 0 ## 8 2 3 1 ## 9 2 4 1 ## 10 2 5 1 ## 11 3 1 0 ## 12 3 2 0 ## 13 3 3 0 ## 14 3 4 1 ## 15 3 5 1 ## 16 4 1 0 ## 17 4 2 0 ## 18 4 3 0 ## 19 4 4 0 ## 20 4 5 1 we see the flat grid of for 1 there are each 1, then 2, then and so on to 4. Then this is stacked on four 2’s, on top of four 3’s, and four 4’s. This is the same kind of operation the gather() function performed in the previous section. We then add one for line to identify that the value column are not only the 1’s and 0’s from the tibble, but they are to be characters. This allows our plot to identify them as categories and thus fillable with colors. With all of that we make the plot. The `ggplot() statement sets up the canvas with x and y axes and the value fill for each of the x by possibilities in the bag of marbles-voters. Laayered in are points with a particular shape and size, fill colors, and scaling details. A theme() call removes the unnecessary (for our purpose) legend. d &lt;- tibble(p1 = 0, p2 = rep(1:0, times = c(1, 3)), p3 = rep(1:0, times = c(2, 2)), p4 = rep(1:0, times = c(3, 1)), p5 = 1) # d %&gt;% set_names(1:5) %&gt;% mutate(x = 1:4) %&gt;% pivot_longer(-x, names_to = &quot;conjecture&quot;) %&gt;% mutate(value = value %&gt;% as.character()) %&gt;% ggplot(aes(x = x, y = conjecture, fill = value)) + geom_point(shape = 21, size = 5) + scale_fill_manual(values = c(&quot;blue&quot;, &quot;red&quot;)) + scale_x_discrete(NULL, breaks = NULL) + theme(legend.position = &quot;none&quot;) The tidyr::pivot_longer() function is an updated variant of gather(). Just like Excel pivot tables, pivot_longer() allows for very creative reshaping and aggregation of data. These articles, here and here (“Pivot Data from Wide to Long Pivot_longer” 2020; “Pivoting” 2020), provide more examples. Here’s the basic structure of the conjectures per marble-voter draw. tibble(draw = 1:3, marbles = 4) %&gt;% mutate(possibilities = marbles^draw) %&gt;% knitr::kable() draw marbles possibilities 1 4 4 2 4 16 3 4 64 We have a table and visual of the 5 conjedctures in hand. Now we make another flat (longer format again) tibble that will have 84 rows of all of the combinations of the three draws of blue, red, and blue in the example. At the first draw there are 4 marble-voters. At the second draw there are \\(4^2 = 16\\) potential marble-voter draws. At the third draw each of these 16 has 4 marble-voters in the bag as possibilities or \\(4^3=64\\). These are positions on a tree with 4 nodes blossoming into 16 nodes, further growing into 64 nodes for a total of 84 nodes. We then linearly interpolate equally spaced nodes from node 1 to node 84. position &lt;- c((1:4^1)/4^0, (1:4^2)/4^1, (1:4^3)/4^2) position ## [1] 1.0000 2.0000 3.0000 4.0000 0.2500 0.5000 0.7500 1.0000 1.2500 1.5000 1.7500 2.0000 ## [13] 2.2500 2.5000 2.7500 3.0000 3.2500 3.5000 3.7500 4.0000 0.0625 0.1250 0.1875 0.2500 ## [25] 0.3125 0.3750 0.4375 0.5000 0.5625 0.6250 0.6875 0.7500 0.8125 0.8750 0.9375 1.0000 ## [37] 1.0625 1.1250 1.1875 1.2500 1.3125 1.3750 1.4375 1.5000 1.5625 1.6250 1.6875 1.7500 ## [49] 1.8125 1.8750 1.9375 2.0000 2.0625 2.1250 2.1875 2.2500 2.3125 2.3750 2.4375 2.5000 ## [61] 2.5625 2.6250 2.6875 2.7500 2.8125 2.8750 2.9375 3.0000 3.0625 3.1250 3.1875 3.2500 ## [73] 3.3125 3.3750 3.4375 3.5000 3.5625 3.6250 3.6875 3.7500 3.8125 3.8750 3.9375 4.0000 Not only that we should label each node level as draw 1, 2, and 3. For 84 nodes we have to match the first draw of 4 marble-voters, each of which again blossoms into 4 for a total of 16 potential marble-voters, and on to the third draw with 64 marble voters for a total of 84 again. draw &lt;- rep(1:3, times = c(4^1, 4^2, 4^3)) draw ## [1] 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## [44] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 In a plot we will fill these categorically with blue and red. First we get the conjecture sequence of blue, followed by 3 red marbles in the bag. We remember that this is one of five conjectures. fill &lt;- rep(c(&quot;b&quot;, &quot;r&quot;), times = c(1, 3)) fill ## [1] &quot;b&quot; &quot;r&quot; &quot;r&quot; &quot;r&quot; Now we mash these four marbles with the 84 draws and positions. fill &lt;- rep(c(&quot;b&quot;, &quot;r&quot;), times = c(1, 3)) %&gt;% rep(., times = c(4^0 + 4^1 + 4^2)) fill ## [1] &quot;b&quot; &quot;r&quot; &quot;r&quot; &quot;r&quot; &quot;b&quot; &quot;r&quot; &quot;r&quot; &quot;r&quot; &quot;b&quot; &quot;r&quot; &quot;r&quot; &quot;r&quot; &quot;b&quot; &quot;r&quot; &quot;r&quot; &quot;r&quot; &quot;b&quot; &quot;r&quot; &quot;r&quot; &quot;r&quot; &quot;b&quot; ## [22] &quot;r&quot; &quot;r&quot; &quot;r&quot; &quot;b&quot; &quot;r&quot; &quot;r&quot; &quot;r&quot; &quot;b&quot; &quot;r&quot; &quot;r&quot; &quot;r&quot; &quot;b&quot; &quot;r&quot; &quot;r&quot; &quot;r&quot; &quot;b&quot; &quot;r&quot; &quot;r&quot; &quot;r&quot; &quot;b&quot; &quot;r&quot; ## [43] &quot;r&quot; &quot;r&quot; &quot;b&quot; &quot;r&quot; &quot;r&quot; &quot;r&quot; &quot;b&quot; &quot;r&quot; &quot;r&quot; &quot;r&quot; &quot;b&quot; &quot;r&quot; &quot;r&quot; &quot;r&quot; &quot;b&quot; &quot;r&quot; &quot;r&quot; &quot;r&quot; &quot;b&quot; &quot;r&quot; &quot;r&quot; ## [64] &quot;r&quot; &quot;b&quot; &quot;r&quot; &quot;r&quot; &quot;r&quot; &quot;b&quot; &quot;r&quot; &quot;r&quot; &quot;r&quot; &quot;b&quot; &quot;r&quot; &quot;r&quot; &quot;r&quot; &quot;b&quot; &quot;r&quot; &quot;r&quot; &quot;r&quot; &quot;b&quot; &quot;r&quot; &quot;r&quot; &quot;r&quot; All together we have this tibble. While this is quite a job to think through and carve out this hierarchical data tree we gain insight into how data is shaped and molded to our purposes. d &lt;- tibble(position = c((1:4^1)/4^0, (1:4^2)/4^1, (1:4^3)/4^2), draw = rep(1:3, times = c(4^1, 4^2, 4^3)), fill = rep(c(&quot;b&quot;, &quot;r&quot;), times = c(1, 3)) %&gt;% rep(., times = c(4^0 + 4^1 + 4^2))) d ## # A tibble: 84 x 3 ## position draw fill ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 1 b ## 2 2 1 r ## 3 3 1 r ## 4 4 1 r ## 5 0.25 2 b ## 6 0.5 2 r ## 7 0.75 2 r ## 8 1 2 r ## 9 1.25 2 b ## 10 1.5 2 r ## # ... with 74 more rows We now have Solomon Kurz’s initial plot. We notice he put in 3 breaks in the otherwise continuous scale that is position. d %&gt;% ggplot(aes(x = position, y = draw, fill = fill)) + geom_point(shape = 21, size = 3) + scale_fill_manual(values = c(&quot;blue&quot;, &quot;red&quot;)) + scale_y_continuous(breaks = 1:3) + theme(legend.position = &quot;none&quot;, panel.grid.minor = element_blank()) How do we draw lines from one draw (level in the hierarchy) to another? Again a tibble comes to our aid. Line segments in plots are simply beginning and ending x and y coordinates.The first of two tibbles will connect the positions from the first to the second draw. The second will connect the 16 second draw positions to the 64 third draw positions. The y coordinates are easy. The x coordinates are a little more daunting. But we have already computed these before in making the tibble positions. We are sure to notice the each = 4 detail for the x coordinates as the nodes fan out from draw to draw. # these will connect the dots from # the first and second draws lines_1 &lt;- tibble(x = rep((1:4), each = 4), xend = ((1:4^2)/4), y = 1, yend = 2) lines_1 ## # A tibble: 16 x 4 ## x xend y yend ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.25 1 2 ## 2 1 0.5 1 2 ## 3 1 0.75 1 2 ## 4 1 1 1 2 ## 5 2 1.25 1 2 ## 6 2 1.5 1 2 ## 7 2 1.75 1 2 ## 8 2 2 1 2 ## 9 3 2.25 1 2 ## 10 3 2.5 1 2 ## 11 3 2.75 1 2 ## 12 3 3 1 2 ## 13 4 3.25 1 2 ## 14 4 3.5 1 2 ## 15 4 3.75 1 2 ## 16 4 4 1 2 # these will connect the dots from # the second and third draws lines_2 &lt;- tibble(x = rep(((1:4^2)/4), each = 4), xend = (1:4^3)/(4^2), y = 2, yend = 3) lines_2 ## # A tibble: 64 x 4 ## x xend y yend ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.25 0.0625 2 3 ## 2 0.25 0.125 2 3 ## 3 0.25 0.188 2 3 ## 4 0.25 0.25 2 3 ## 5 0.5 0.312 2 3 ## 6 0.5 0.375 2 3 ## 7 0.5 0.438 2 3 ## 8 0.5 0.5 2 3 ## 9 0.75 0.562 2 3 ## 10 0.75 0.625 2 3 ## # ... with 54 more rows We now use the lines_1 and lines_2 data in the plot with two geom_segment() functions layered into the initial plot. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_point(aes(fill = fill), shape = 21, size = 3) + scale_fill_manual(values = c(&quot;blue&quot;, &quot;red&quot;)) + scale_y_continuous(breaks = 1:3) + theme(legend.position = &quot;none&quot;, panel.grid.minor = element_blank()) We generated the values for position (i.e., the \\(x\\)-axis) al right justified to the position number. To center the nodes for the first draw we simply will subtract 0.5 from each node. In a slightly trickier move, noting there are four marble-voter possibilities in this conjecture we will subtract \\(0.50/4=0.125\\) for draw 2, and \\((0.05/4)/4=0.125/4=0.03125\\) for draw 3. The ifelse() function will allow us to control the three conditions across the positions. If anything else, this is great control practice especially with piping. We should notice the use of the \\(4^0=1,\\,4^1=4,\\,4^2=16\\) sequence. d &lt;- d %&gt;% mutate(denominator = ifelse(draw == 1, 0.5, ifelse(draw == 2, 0.5/4, 0.5/4^2))) %&gt;% mutate(position = position - denominator) The same logic will work for lines_1 and lines_2. lines_1 &lt;- lines_1 %&gt;% mutate(x = x - 0.5, xend = xend - 0.5/4^1) lines_2 &lt;- lines_2 %&gt;% mutate(x = x - 0.5/4^1, xend = xend - 0.5/4^2) lines_1 ## # A tibble: 16 x 4 ## x xend y yend ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.5 0.125 1 2 ## 2 0.5 0.375 1 2 ## 3 0.5 0.625 1 2 ## 4 0.5 0.875 1 2 ## 5 1.5 1.12 1 2 ## 6 1.5 1.38 1 2 ## 7 1.5 1.62 1 2 ## 8 1.5 1.88 1 2 ## 9 2.5 2.12 1 2 ## 10 2.5 2.38 1 2 ## 11 2.5 2.62 1 2 ## 12 2.5 2.88 1 2 ## 13 3.5 3.12 1 2 ## 14 3.5 3.38 1 2 ## 15 3.5 3.62 1 2 ## 16 3.5 3.88 1 2 lines_2 ## # A tibble: 64 x 4 ## x xend y yend ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.125 0.0312 2 3 ## 2 0.125 0.0938 2 3 ## 3 0.125 0.156 2 3 ## 4 0.125 0.219 2 3 ## 5 0.375 0.281 2 3 ## 6 0.375 0.344 2 3 ## 7 0.375 0.406 2 3 ## 8 0.375 0.469 2 3 ## 9 0.625 0.531 2 3 ## 10 0.625 0.594 2 3 ## # ... with 54 more rows Now the plot looks centered and more like a drafting dream come true. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_point(aes(fill = fill), shape = 21, size = 3) + scale_fill_manual(values = c(&quot;blue&quot;, &quot;red&quot;)) + scale_y_continuous(breaks = 1:3) + theme(legend.position = &quot;none&quot;, panel.grid.minor = element_blank()) Last and not the least visually, let’s wrap the x-axis about a circle by transforming from a rectangular to a polar coordinate system with the coord_polar() layer. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_point(aes(fill = fill), shape = 21, size = 4) + scale_fill_manual(values = c(&quot;blue&quot;, &quot;red&quot;)) + scale_x_continuous(NULL, limits = c(0, 4), breaks = NULL) + scale_y_continuous(NULL, limits = c(0.75, 3), breaks = NULL) + coord_polar() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) We have to recall that the 1 blue and three red in the middle of this mandela is but one conjecture. There are 4 others, only two of which are non-trivial. The whole point of this execise is the identify by brute force enumeration the only three choices that allow the data to be consistent with the conjecture. To illustrate this Kurz adds an index called remain to indicate only those logically consistent paths after each choice for nodes and lines (edges) of this graph. The remain indicator is 1 if the draw (data) is logically consistent with the conjecture (hypothesis) and 0 if not. We will print out d to illustrate. lines_1 &lt;- lines_1 %&gt;% mutate(remain = c(rep(0:1, times = c(1, 3)), rep(0, times = 4 * 3))) lines_2 &lt;- lines_2 %&gt;% mutate(remain = c(rep(0, times = 4), rep(1:0, times = c(1, 3)) %&gt;% rep(., times = 3), rep(0, times = 12 * 4))) d &lt;- d %&gt;% mutate(remain = c(rep(1:0, times = c(1, 3)), rep(0:1, times = c(1, 3)), rep(0, times = 4 * 4), rep(1:0, times = c(1, 3)) %&gt;% rep(., times = 3), rep(0, times = 12 * 4))) d ## # A tibble: 84 x 5 ## position draw fill denominator remain ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.5 1 b 0.5 1 ## 2 1.5 1 r 0.5 0 ## 3 2.5 1 r 0.5 0 ## 4 3.5 1 r 0.5 0 ## 5 0.125 2 b 0.125 0 ## 6 0.375 2 r 0.125 1 ## 7 0.625 2 r 0.125 1 ## 8 0.875 2 r 0.125 1 ## 9 1.12 2 b 0.125 0 ## 10 1.38 2 r 0.125 0 ## # ... with 74 more rows And now Kurz’s piece de resistance. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend, alpha = remain %&gt;% as.character()), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend, alpha = remain %&gt;% as.character()), size = 1/3) + geom_point(aes(fill = fill, alpha = remain %&gt;% as.character()), shape = 21, size = 4) + # it&#39;s the alpha parameter that makes # elements semitransparent scale_fill_manual(values = c(&quot;blue&quot;, &quot;red&quot;)) + scale_alpha_manual(values = c(1/5, 1)) + scale_x_continuous(NULL, limits = c(0, 4), breaks = NULL) + scale_y_continuous(NULL, limits = c(0.75, 3), breaks = NULL) + coord_polar() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) The alpha attribute in the geom_point() governs the opacity of each filled circle (shape = 21). When remain as a categorical variable (%&gt;% as.character()) is 1 the default 100% opacity is rendered. Otherwise a lesser opacity will render and thus we see the lighter elements. All in all this exercise allows us to stretch our data creation, reshaping, and transformation muscles. It also shows the effort needed to create compelling visualizations of the data for consumption by decision makers. A useful practice exercise is to use this model to build the visualizations of the other two conjectures. References "],
["references.html", "References", " References "]
]
