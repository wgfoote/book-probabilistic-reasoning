[
["sampling-the-hypothetical.html", "Chapter 7 Sampling the Hypothetical 7.1 On your marks … 7.2 Brute force 7.3 The least of the squares 7.4 Look: up in the sky it’s a … Session Information", " Chapter 7 Sampling the Hypothetical 7.1 On your marks … Richard McElreath starts in earnest with Sampling the Imaginary. Insofar as the imaginary, as things, emanate from our imagination, and thus meets reality somewhere, then this makes sense epistemologically. The imaginary is otherwise a very ontological thing in our minds, and we might even share that thing.1 (???) notes that black swans were imaginary, of course until they were discovered in Australia. The imaginary mental being lurks in our consciousness. An example is the exclamation by DiFenetti ((???)) that PROBABILITY DOES NOT EXIST. So true! This is a concept that many share but it is a construct in our minds, like every single model in mental existence, even those written on very real paper. 7.2 Brute force A popular, easy both to imagine and construct, approach to mashing data into hypotheses is the grid approximation approach. The grid we imagine here is are the 101 possible and quite hypothetical probabilities of a positive result in, say, testing for a novel virus. In this scenario we are sampling from all 8 zip coded in the Bronx. Suppose we find 2 positive zip codes. Following Kurz’s lead we tibble into a solution. library(tidyverse) ## -- Attaching packages -------------------------------------------------- tidyverse 1.3.0 -- ## v ggplot2 3.3.2 v purrr 0.3.4 ## v tibble 3.0.3 v dplyr 1.0.1 ## v tidyr 1.1.1 v stringr 1.4.0 ## v readr 1.3.1 v forcats 0.5.0 ## -- Conflicts ----------------------------------------------------- tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(rethinking) ## Loading required package: rstan ## Loading required package: StanHeaders ## rstan (Version 2.21.2, GitRev: 2e1f913d3ca3) ## For execution on a local, multicore CPU with excess RAM we recommend calling ## options(mc.cores = parallel::detectCores()). ## To avoid recompilation of unchanged Stan programs, we recommend calling ## rstan_options(auto_write = TRUE) ## Do not specify &#39;-march=native&#39; in &#39;LOCAL_CPPFLAGS&#39; or a Makevars file ## ## Attaching package: &#39;rstan&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract ## Loading required package: parallel ## rethinking (Version 2.12) ## ## Attaching package: &#39;rethinking&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## map ## The following object is masked from &#39;package:stats&#39;: ## ## rstudent n &lt;- 1000 n_success &lt;- 6 n_trials &lt;- 8 ( d &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = n), # note we&#39;re still using a flat uniform prior prior = 1) %&gt;% mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) %&gt;% mutate(posterior = (likelihood * prior) / sum(likelihood * prior)) ) ## # A tibble: 1,000 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0. 0. ## 2 0.00100 1 2.81e-17 2.53e-19 ## 3 0.00200 1 1.80e-15 1.62e-17 ## 4 0.00300 1 2.04e-14 1.84e-16 ## 5 0.00400 1 1.14e-13 1.03e-15 ## 6 0.00501 1 4.36e-13 3.93e-15 ## 7 0.00601 1 1.30e-12 1.17e-14 ## 8 0.00701 1 3.27e-12 2.94e-14 ## 9 0.00801 1 7.27e-12 6.55e-14 ## 10 0.00901 1 1.47e-11 1.32e-13 ## # ... with 990 more rows summary( d ) ## p_grid prior likelihood posterior ## Min. :0.00 Min. :1 Min. :0.000000 Min. :0.000e+00 ## 1st Qu.:0.25 1st Qu.:1 1st Qu.:0.003022 1st Qu.:2.722e-05 ## Median :0.50 Median :1 Median :0.064970 Median :5.853e-04 ## Mean :0.50 Mean :1 Mean :0.111000 Mean :1.000e-03 ## 3rd Qu.:0.75 3rd Qu.:1 3rd Qu.:0.220190 3rd Qu.:1.984e-03 ## Max. :1.00 Max. :1 Max. :0.311462 Max. :2.806e-03 The job is almost done and now for the core reason we are here. We sample these hypothetical values of the probability of a single positive test, and visualize our handiwork. # how many samples would you like? n_samples &lt;- 10000 # make it reproducible set.seed(42) samples &lt;- d %&gt;% sample_n( size = n_samples, weight = posterior, replace = T ) glimpse(samples) ## Rows: 10,000 ## Columns: 4 ## $ p_grid &lt;dbl&gt; 0.5345345, 0.7307307, 0.8528529, 0.8168168, 0.8698699, 0... ## $ prior &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ likelihood &lt;dbl&gt; 0.14151014, 0.30908352, 0.23329550, 0.27904694, 0.205418... ## $ posterior &lt;dbl&gt; 0.0012748662, 0.0027845362, 0.0021017613, 0.0025139364, ... # y_label &lt;- &quot;h = proportion of positive tests&quot; x_label &lt;- &quot;sample index&quot; samples %&gt;% mutate(sample_number = 1:n()) %&gt;% # Here&#39;s the cloud of unknowning ggplot(aes( x = sample_number, y = p_grid) ) + geom_point( alpha = 1/10 ) + scale_y_continuous( y_label, limits = c(0, 1) ) + xlab( x_label ) Let’s transform from the cloud of unknowing into the frequency of ways in which 8 zip codes can plausibly have 2 positive results. Let’s also add a vertical line to indicate the mode of this posterior distribution. The tidybayes package has a slew of useful summarizing statistics, including the Mode() which we use below to find the proportion of tests that correspond to the posterior’s mode. The Maximum A Posteriori (aka MAP) hypothesis is just the mode, that is, the most frequently occurring value of the parameter \\(p\\), This will be one point estimate we can report. library(plotly) ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout library(tidybayes) p_MAP &lt;- Mode(samples$p_grid) title &lt;- &quot;Bronx Zip Code Tests&quot; x_label &lt;- &quot;proportion of zip codes testing positive&quot; y_label &lt;- &quot;posterior density&quot; plt &lt;- samples %&gt;% ggplot(aes(x = p_grid)) + geom_density(fill = &quot;blue&quot;, alpha = 0.3) + scale_x_continuous(x_label, limits = c(0, 1)) + geom_vline(xintercept=p_MAP, color = &quot;orange&quot;) + annotate( &quot;text&quot;, x = 0.50, y = 2, label = paste0(&quot;MAP = &quot;, round(p_MAP, 4)) ) + ylab(y_label) + xlab(x_label) + ggtitle(title) ggplotly(plt) Other measures of tendency include the quantiles. We can summarize our testing results in a number of ways depending on the purpose of the report and the desires of the client. Here is one way using the summarize function the tidyverse’s dplyr package along with a few tidybayes functions, of course, for good measure. 7.2.1 Skin in the game We have let \\(h = p\\) be our hypothesized proportion of zip codes testing positive. Now we suppose we are at a game of chance (they do exist in the Bronx!) and guess \\(d = positive\\) for the next text. The house will pay us $1 if we guess exactly right. If we guess wrong we get docked by an amount proportional to the absolute value of the distance \\(d - p\\) we deviate. We now have a loss function. We also now call \\(d\\) a decision. But it is a decision based on a set of hypotheses about decisions, our prior beliefs about those decisions, and driven by the likelihood of seeing decision outcomes in data, a posterior implication. The implication is the plausibility of those decisions logically compatible with experience of observing decision outcomes. Following de Moivre’s (Moivre (1756)) doctrine of chance we compute our expected loss. This approach constructs a criterion, also known as an objective function. The absolute value function is also known as the check function and is used by (???) to build a whole family of quantile regression models the parameters of which are solutions to linear programming models. Here is what the loss objective function looks like. guess_d &lt;- 0.5 m &lt;- guess_d X &lt;- seq(0, 1, length.out = 100) Y &lt;- ifelse(X &lt; m, -1, ifelse(X &gt; m, 1, 0)) XY &lt;- tibble(X = X, Y = Y) plt &lt;- ggplot(XY, aes(x = X, y = Y)) + geom_line(color = &quot;blue&quot;, size = 2)+ geom_vline(xintercept = m, linetype = &quot;dashed&quot;) + geom_point(x = m, y = 0, color = &quot;red&quot;, size = 3) + geom_hline(yintercept = 0.0) + geom_point(x = m, y = -1, color = &quot;red&quot;, size = 3) + geom_point(x = m, y = +1, color = &quot;red&quot;, size = 3) + geom_segment(aes(x = m, y = -1+.03, xend = m, yend = 1-.03 ), color = &quot;white&quot;, size = 3, linetype = &quot;dashed&quot;) + geom_point(x = m, y = -1, color = &quot;red&quot;, size = 3) + geom_point(x = m, y = +1, color = &quot;red&quot;, size = 3) + xlab(&quot;Y&quot;) + ylab(&quot;f(Y,m) = |Y-m|&quot;) ggplotly(plt) Here we have deviations of a p_grid value \\(m\\) from a range of possible guesses \\(Y\\). From high school algebra we might remember that the rate of change of the absolute value function is this v-shaped beast. It is a beast because the derivative, that is, the slope, is not defined at \\(m\\). Instead we must gingerly approach the value from the left and the right. Here finally is the check function for a guess of 0.75, as in our zip code example. m &lt;- 0.75 X &lt;- seq(0, 1, length.out = 100) Y &lt;- abs(X-m) XY &lt;- data_frame(X = X, Y = Y) ## Warning: `data_frame()` is deprecated as of tibble 1.1.0. ## Please use `tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. plt &lt;- ggplot(XY, aes(x = X, y = Y)) + geom_line(color = &quot;blue&quot;, size = 2.0) + geom_vline(xintercept = m, linetype = &quot;dashed&quot;) + geom_point(x = m, y = 0, color = &quot;red&quot;, size = 3) + geom_hline(yintercept = 0.0) + xlab(&quot;Y&quot;) + ylab(&quot;f &#39; (Y,m) = |Y-m|&quot;) ggplotly(plt) Let’s guess that \\(d=0.5\\) and try this value on our simulated grid by computed the weighted average of losses with weights equal to the posterior distribution. We can use the summarize() function from the dplyr package in the tidyverse ecosystem. appl d %&gt;% summarize(`expected loss` = sum(posterior * abs(0.5 - p_grid))) ## # A tibble: 1 x 1 ## `expected loss` ## &lt;dbl&gt; ## 1 0.213 McElreath uses the sapply() function from base R to vectorize a calculation of losses using a loss function. The purrr::map() function essentially does the same job and we can see more applications of the purrr package (Henry and Wickham 2020), which is itself part of the tidyverse and of the map() family here or here or here. The map() will take an input and run this input through whatever function we might dream up. So let’s make up a make_loss() function out of the summarize() experiment above. Typically this is how we construct functions. We first run through a set of routimes. Once we are satisfied with the behavior of those routines, we enshrine them in functions for repeated use. First a mutate() creates a vector of weighted losses, then the summarize() verb finishes the calculate of the sumproducts. make_loss &lt;- function(guess_d) { d %&gt;% mutate(loss = posterior * abs(guess_d - p_grid)) %&gt;% summarise(weighted_average_loss = sum(loss)) } # TEST! make_loss(0.75) ## # A tibble: 1 x 1 ## weighted_average_loss ## &lt;dbl&gt; ## 1 0.115 We always test our handiwork, and as we hoped (expected?) we get the same result as before. It seems our function works just fine. ( loss &lt;- d %&gt;% select(p_grid) %&gt;% rename(decision = p_grid) %&gt;% mutate(weighted_average_loss = purrr::map(decision, make_loss)) %&gt;% unnest(weighted_average_loss) ) ## # A tibble: 1,000 x 2 ## decision weighted_average_loss ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0.700 ## 2 0.00100 0.699 ## 3 0.00200 0.698 ## 4 0.00300 0.697 ## 5 0.00400 0.696 ## 6 0.00501 0.695 ## 7 0.00601 0.694 ## 8 0.00701 0.693 ## 9 0.00801 0.692 ## 10 0.00901 0.691 ## # ... with 990 more rows Let’s identify the optimal, that is, the minimum loss using the filter() verb. In this way we can build a vertical line to indicate the optimal loss. A ribbon will fill the area under the loss curve. We can interact with the graph using the ggploty() function on the ggplot() object plt. # this will help us find the x and y coordinates for the minimum value min_loss &lt;- loss %&gt;% filter(weighted_average_loss == min(weighted_average_loss)) %&gt;% as.numeric() min_label &lt;- list( paste0( &quot;minimum loss = &quot;, round(min_loss[2], 2) ), paste0( &quot;decision = &quot;, round(min_loss[1], 2) ) ) # update the plot plt &lt;- loss %&gt;% ggplot(aes(x = decision)) + geom_ribbon(aes(ymin = 0, ymax = weighted_average_loss), fill = &quot;blue&quot;, alpha = 0.30) + geom_vline(xintercept = min_loss[1], color = &quot;red&quot;, linetype = 3) + geom_hline(yintercept = min_loss[2], color = &quot;red&quot;, linetype = 3) + annotate(&quot;text&quot;, x = rep(min_loss[1], 2 ), y = c( 0.4, 0.45), label = min_label) + ylab(&quot;expected proportional absolute deviation loss&quot;) + theme(panel.grid = element_blank()) ggplotly(plt) We saved the exact minimum value as min_loss[1], which is 0.7137137. Within sampling error, this is the posterior 50th quantile, the median, as depicted by our samples, except for the sampling error. It is the median as it is the optimal choice of decision based on the absolute deviation function. samples %&gt;% summarise(posterior_50 = quantile(p_grid, 0.50)) ## # A tibble: 1 x 1 ## posterior_50 ## &lt;dbl&gt; ## 1 0.715 We will use the quantile() to define boundaries within the posterior distribution. This function also provides thresholds for extreme values in the distribution, a measure call the Value-at-risk in finance. 7.2.1.1 An Aside Even more interesting is the idea we can find a middling measure that minimizes the sum of absolute deviations of data around an as yet unspecified parameter (too many \\(m\\)!). \\[ SAD = \\Sigma_{i=1}^5 |Y_i - m| \\] Yes, it is SAD, the sum of absolute deviations. This is our foray into rank-oder statistics, quite a bit different in nature than the arithmetic mean of \\(SSE\\) fame, whic we will get to in due time. We get to basic counting when we try to mind the \\(m\\) that minimizes SAD. To illustrate this suppose our data is all positive (ratio data in fact). If \\(m=0.5\\) then the function \\[ f(Y;m) = |Y-m| \\] has this appearance, the so-called check function. m &lt;- 0.5 X &lt;- seq(0, 1, length.out = 100) Y &lt;- abs(X-m) XY &lt;- data_frame(X = X, Y = Y) p &lt;- ggplot(XY, aes(x = X, y = Y)) + geom_line(color = &quot;blue&quot;, size = 2.0) + geom_vline(xintercept = m, linetype = &quot;dashed&quot;) + geom_point(x = m, y = 0, color = &quot;red&quot;, size = 3) + geom_hline(yintercept = 0.0) + xlab(&quot;Y&quot;) + ylab(&quot;f(Y,m) = |Y-m|&quot;) ggplotly(p) Intuitively, half the graph seems to be the left of \\(m=0.5\\), the other have is to the right. Let’s look at the first derivative of the check function with respect to changes in \\(m\\), just like we did with each term in \\(SSE\\). Notice that the (eyeballed) rise over run, i..e., slope, before \\(m=5\\) is -1, and after it is +1. At \\(m=0.5\\) there is no slope that’s immediately meaningful. We have two cases to consider. First \\(Y\\) can be less than or equal to \\(m\\) so that \\(Y-m \\leq 0\\). In this case \\[ \\frac{d\\,\\,(Y-m)_{\\leq0}}{dY} = -1 \\] This corresponds exactly to negatively sloped line accumulating like the seive of Erasthenes into our supposed \\(m=0.5\\) in the plot. Second, \\(Y\\) can be greater than or equal to \\(m\\) so that \\(Y-m \\geq 0\\). In this case \\[ \\frac{d\\,\\,(Y-m)_{\\geq0}}{dY} = +1 \\] also correpsonding to the positively sloped portion of the graph. Another graph is in order to imagine this derivative. m &lt;- 0.5 X &lt;- seq(0, 1, length.out = 100) Y &lt;- ifelse(X &lt; m, -1, ifelse(X &gt; m, 1, 0)) XY &lt;- data_frame(X = X, Y = Y) p &lt;- ggplot(XY, aes(x = X, y = Y)) + geom_line(color = &quot;blue&quot;, size = 2)+ geom_vline(xintercept = m, linetype = &quot;dashed&quot;) + geom_point(x = m, y = 0, color = &quot;red&quot;, size = 3) + geom_hline(yintercept = 0.0) + geom_point(x = m, y = -1, color = &quot;red&quot;, size = 3) + geom_point(x = m, y = +1, color = &quot;red&quot;, size = 3) + geom_segment(aes(x = m, y = -1+.03, xend = m, yend = 1-.03 ), color = &quot;white&quot;, size = 3, linetype = &quot;dashed&quot;) + geom_point(x = m, y = -1, color = &quot;red&quot;, size = 3) + geom_point(x = m, y = +1, color = &quot;red&quot;, size = 3) + xlab(&quot;Y&quot;) + ylab(&quot;f(Y,m) = |Y-m|&quot;) ggplotly(p) It’s all or nothing for the derivative, a classic step function. We use this fact in the following (near) finale in our search for \\(m\\). Back to \\(SAD\\). We are looking for the \\(m\\) that minimizes \\(SAD\\): \\[ SAD = \\Sigma_{i=1}^N |Y_i - m| = |Y_1-m| + \\ldots + |Y_N-m| \\] If we take the derivative of \\(SAD\\) with respect to \\(Y\\) data points, we get \\(N\\) minus 1s and \\(N\\) plus ones in our sum because each and every \\(|Y_i-m|\\) could either be greater than or equal to \\(m\\) or less than or equal to \\(m\\), we just just don’t know which, so we need to consider both cases at once. We also don’t know off hand how many data points are to the left or the right of the value of \\(m\\) that minimizes \\(SAD\\)! Let’s play a little roulette and let \\(L\\) be the number of (unknown) points to the left of \\(m\\) and \\(R\\) points to the right. Then \\(SAD\\) looks like it is split into two terms, just like the two intervals leading up to and away from the red dot at the bottom of the check function. \\[ SAD = \\Sigma_{i=1}^R |Y_i - m| + \\Sigma_{i=1}^R |Y_i - m| = (|Y_1-m| + \\ldots + |Y_L-m|) + (|Y_1-m| + \\ldots + |Y_R-m|) \\] \\[ \\frac{d\\,\\,SAD}{dY} = \\Sigma_{i=1}^L (-1) + \\Sigma_{i=1}^R (+1) = (-1)L+ (+1)R \\] When we set this result to zero for the first order condition for an optimum we get a possibly strange, but appropriate result. The tradeoff between left and right must offset one another exactly. \\[ (-1)L + (+1)R = 0 \\] \\[ L = R \\] Whatever number of points are to the left must also be to the right of \\(m\\). If \\(L\\) points also include \\(m\\), then \\(L/N\\geq1/2\\) as well as for the \\(R\\) points if they include \\(m\\) so that \\(R/N\\geq1/2\\). We have arrived at what a median is. Now we come up with a precise statement of the middle of a data series, the notorious median. We let \\(P()\\) be the proportion of data points at and above (if \\(Y \\geq M\\)) or at and below (\\(Y \\leq m\\)). THe median, \\(m\\), is the first time a data point in a data series reaches both \\(P(Y \\leq m) \\geq 1/2\\) (from minimum data point) and \\(P(Y \\geq m) \\geq 1/2\\) (from the maximum data point) It’s logic again with these conjunctive statements. That definition will work for us whether each data point is equally likely (\\(1/N\\)) as in a so-called uninformative prior or from grouped data with symmetric or skewed relative frequency distributions. 7.3 The least of the squares What if Kurz and McElreath gang up on us and dock our ante and subsequent winnings with a loss proportional to the square of the deviation of our decision (guess) from the grid or \\((d - p)^2\\). What does this suggest? We have a new payoff in our make_loss() function to deal with. # ammend our loss function make_quad_loss &lt;- function(guess_d) { d %&gt;% mutate(loss = posterior * (guess_d - p_grid)^2) %&gt;% summarise(weighted_average_loss = sum(loss)) } make_loss(0.75) ## # A tibble: 1 x 1 ## weighted_average_loss ## &lt;dbl&gt; ## 1 0.115 We reuse the code we know works to solve this problem. # rebuild the loss data loss_quad &lt;- d %&gt;% select(p_grid) %&gt;% rename(decision = p_grid) %&gt;% mutate(weighted_average_loss = purrr::map(decision, make_quad_loss)) %&gt;% unnest(weighted_average_loss) # update to the new minimum loss coordinates min_loss &lt;- loss_quad %&gt;% filter(weighted_average_loss == min(weighted_average_loss)) %&gt;% as.numeric() min_label &lt;- list( paste0( &quot;minimum loss = &quot;, round(min_loss[2], 2) ), paste0( &quot;decision = &quot;, round(min_loss[1], 2) ) ) # update the plot plt &lt;- loss_quad %&gt;% ggplot(aes(x = decision)) + geom_ribbon(aes(ymin = 0, ymax = weighted_average_loss), fill = &quot;blue&quot;, alpha = 0.30) + geom_vline(xintercept = min_loss[1], color = &quot;red&quot;, linetype = 3) + geom_hline(yintercept = min_loss[2], color = &quot;red&quot;, linetype = 3) + annotate(&quot;text&quot;, x = rep(min_loss[1], 2 ), y = c( 0.4, 0.45), label = min_label) + ylab(&quot;expected proportional quadratic loss&quot;) + theme(panel.grid = element_blank()) ggplotly(plt) Based on quadratic loss \\((d - p)^2\\), the exact minimum value is 0.6996997. Within sampling error. There might be a bit of a shock that this is just the arithmetic mean of our samples. An aside will be in order. samples %&gt;% summarise(posterior_mean = mean(p_grid)) ## # A tibble: 1 x 1 ## posterior_mean ## &lt;dbl&gt; ## 1 0.700 The arithmetic mean takes no notice at all of the possibility that different outcomes have different weights. It is this starting position that often frequentists, including me in some realizations, can fail to notice the shape of data, always as that shape is really formed by the knower-analyst’s assumptions. 7.3.0.1 Yet another aside This plot depicts the sum of squared deviations for a grid of potential values of what the data points deviate from, \\(m\\). Use of such a criterion allows us a clear and in this case unique calculation of the best linear estimator for the mean. We hover over the graph and brush over the area around bottom of the function a little below the median. price &lt;- rnorm( 300, 0.5, 0.15 ) m &lt;- seq(0, 1, length.out = 300) SSE &lt;- m for (i in 1:length(m)) { SSE[i] &lt;- t(price - m[i]) %*% (price - m[i]) } opt_plot &lt;- tibble(m = m, SSE = SSE) SSE_min_index &lt;- SSE == min(SSE) SSE_min &lt;- SSE[SSE_min_index] m_min &lt;- m[SSE_min_index] p &lt;- ggplot(opt_plot, aes(x = m, y = SSE)) + geom_line(color = &quot;blue&quot;, size = 1.25) + geom_point(x = m_min, y = SSE_min, color = &quot;red&quot;, size = 4.0) + geom_segment(x = 0, y = SSE_min+5, xend = m_min, yend = SSE_min, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + geom_segment(x = m_min++5, y = 0, xend = m_min, yend = SSE_min, color = &quot;red&quot;, linetype = &quot;dashed&quot;) ggplotly(p) Simply putting the cursor on the red dot indicates a solution: \\(m=\\) 1. A bit of calculus confirms the brute force choice of the arithmetic mean that minimizes the sum of squared deivations about the mean. First, the sum of squared errors (deviations) of the \\(X_i\\) data points about a mean of \\(m\\) is \\[ SSE = \\Sigma_{i=1}^5 (Y_i - m)^2 \\] Second, we derive the first derivative of \\(SSE\\) with reapect to \\(m\\), holding all else (e.g., sums of \\(X_i\\)) and set the derivative equal to zero for the first order condition for an optimum. \\[ \\frac{d\\,\\,SSE}{dm} = -2\\left(\\Sigma_{i=1}^5 (Y_i - m)\\right) = 0 \\] Here we used the chain and power rules of differentiation. Third, we solve for \\(m\\) to find \\[ m = \\frac{\\Sigma_{i=1}^5 Y_i}{N}=0.5043659 \\] Close enough for us? This is none other than the arithmetic mean. We can perform similar procedure to get the sample means of the y-intercept \\(b_0\\) and slope \\(b_1\\) of the relationship \\[ Y_i = b_0 + b_1 X_i + e_i \\] where \\(x_i\\) data points try tp explain movements in the \\(Y_i\\) data points. This will be the subject of our next chapter, an excursion into McElreath’s geocentric models. 7.4 Look: up in the sky it’s a … Let’s try Laplace’s quadratic approximation technique instead of the brute force grid. This approach takes advantage of the observation that around the mode of the posterior distribution, the distribution is approximately Gaussian-shaped. A Gaussian shape is just a quadratic combination of the hypotheses. Here is a version for the simplest model that is not binomial (or poisson for that matter). The Laplace quadratic approximation technique has two steps. Find the optimal values of posterior parameters (MAP). Given the optimal value of posterior parameters and their interactions across the data, simulate posterior values of parameters and data predictions. We will begin to control under and over-flow issues (really small and really large numbers) by using the following relationship between the product, here, of two likelihoods, \\(p_1\\) and \\(p_2\\), and the exponent of the sum of logarithms of the two likelihoods. \\[ p_1p_2 = e^{log(p_1)+log(p_2)} \\] We shift gears out of first gear binomial models into the second gear of a simple mean and standard deviation of a set of continuous data like temperature or electrical usage or even square hectares of land versus water. We suppose our data looks like this (simulated to protect the innocent) and pretend we are energy managers who are reviewing a facility’s electricity usage per minute. The dimensions of \\(y\\) are kilo-watt hours per minute. set.seed(4242) y &lt;- rnorm(n = 20, mean = 10, sd = 5) # kilo-watt hours per minute c(mean = mean(y), sd = sd(y)) ## mean sd ## 9.992379 5.679189 That’s the data \\(y_i\\) for \\(i=1 \\cdots 20\\) and its specification in R. Running toy simulation models like this help us to be sure our more realistic models are working correctly. For the priors we will let the mean be normally distributed (Gaussian) and possibly range from 0 to 100, since we do not know any differently at this stage. Normal distributions are symmetric and are almost triangular looking if we squint at them. We let the standard deviation take on a log Gaussian shape, bounded at 0, since standard deviations are always greater than or equal to zero. The lognormal distribution will have a shape that feels a bit exponential, almost chi-squared. But that’s a consideration for later. There’s an assumption we can build in! The full model looks like this: \\[ \\begin{align} y_i &amp; \\sim \\operatorname{Normal}(\\mu, \\sigma) \\\\ \\mu &amp; \\sim \\operatorname{Normal}(0,100) \\\\ \\sigma &amp; \\sim \\operatorname{LogNormal}(0,10) \\\\ \\end{align} \\] The likelihood function for data \\(y_i\\) looks like this: \\[ \\operatorname{Pr}(y_i \\mid \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\operatorname{exp}\\left[\\frac{(y_i - \\mu)^2}{2\\sigma}\\right] \\] Ye gads! But not to worry, this density function is computed in R with dnorm(). The beast of a formula has \\(\\pi\\) in it so it very likely has something to do with circles and trigonometry, again for later, some day. It is definitely easier to use the ~ language above to specify models. We set the feature to take logarithms of this computation with a log=T argument in dnorm(). This function will compute for the two parameterss in p and the data in y, the log posterior density. It we raise this number to the power of \\(e\\) we get the original product of posterior likelihoods. We note how we incorporate the two assumptions about the parameters as priors in the computation. model &lt;- function(p, y) { log_lik &lt;- sum(dnorm(y, p[&quot;mu&quot;], p[&quot;sigma&quot;], log = T)) # the log likelihood log_post &lt;- log_lik + dnorm(p[&quot;mu&quot;], 0, 100, log = T) + dlnorm(p[&quot;sigma&quot;], 0, 10, log = T) return(log_post) } So this works by inserting the data \\(y_i\\) and initial guesses about parameter values into an optimizer that searche across values of mu and sigma for the maximum log-likelihood of seeing all of the data y given a mu and signma. This is a bit different than calculating the arithmetic mean and standard deviation isn’t it? inits &lt;- c(mu = 0, sigma = 1) fit &lt;- optim(inits, model, control = list(fnscale = -1), hessian = TRUE, y = y) fit$par ## mu sigma ## 9.995127 5.397893 fit$hessian ## mu sigma ## mu -0.6865065387 0.0006988561 ## sigma 0.0006988561 -1.4444997696 What we get from this exercise is a lot more than arithmetic means and standard deviations. We are analyst-skeptics. We need to learn something about how the variations in the data reflect in variations in the mean and standard deviation, as well as how the mean and standard deviation end up interacting as well. We get the interactions of the mean and standard deviatino from the hessian matrix computed during the computes the variance-covariance of the solution 9.9951265, 5.3978933. We are not done! Next we use the fit parameters that have located the Maximum A Posteriori values of the two parameters $= $ 9.9951265 and $= $ 5.3978933 to generate posterior predictive samples, plot them, talk about them, and then we are done (for the moment!). If we invert the negative of the hessian matrix, we get back the variance-covariance matrix of the two parameters. Frequentist-while, the square root of the diagonal of this inverted hessian matrix can form the basis of t-tests of the null hypothesis that \\(\\mu = 0\\) or some other target value \\(\\mu_0\\). But we are not frequentists, so simulation-while we return to sampling. par_mean &lt;- fit$par par_varcov &lt;- solve(-fit$hessian) round( par_mean, 2 ) ## mu sigma ## 10.0 5.4 round( par_varcov, 4 ) ## mu sigma ## mu 1.4567 0.0007 ## sigma 0.0007 0.6923 In order to sample predictive posterior views, really hypotheses of what might be, of \\(\\mu\\) and \\(\\sigma\\) as Gaussian variates, we will need to take into account not only the means and variances of the \\(\\mu\\) and \\(\\sigma\\) parameters, but we will need to include how these parameters would vary with one another with covariances 0. The mvtnorm package will come to our aid. First the samples, then add a simulation of predictions for \\(y\\). We will use the coda package to display the distributions this time, since it is a popular way to visualize Monte Carlo simulations of the Markov Chain species. Essentially this is the package that McElreath’s rethinking package employs. library(mvtnorm) samples &lt;- rmvnorm(10000, par_mean, par_varcov) samples &lt;- as.tibble(samples) %&gt;% mutate(prediction = rnorm(n = n(), mu, sigma)) ## Warning: `as.tibble()` is deprecated as of tibble 2.0.0. ## Please use `as_tibble()` instead. ## The signature and semantics have changed, see `?as_tibble`. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. # simplehist(samples %&gt;% select(prediction)) # try coda plots too library(coda) ## ## Attaching package: &#39;coda&#39; ## The following object is masked from &#39;package:rstan&#39;: ## ## traceplot samples_mcmc &lt;- mcmc(samples) densplot(samples_mcmc) Here is a high density interval view of the modes of the three samplings. A 95% interval width allows us to say that the values of posterior parameters and predictions are between lower and upper bounds with 95% compatibility with the data. These are compatibility, plausibility, posterior probability, credibility intervals. That last adjective is for the insurance analysts in the gang. The pivot_longer() is preferred over gather(). The name is automatically generated from column names in the tibble, while value is the new array of sampled parameters and predictions. samples %&gt;% pivot_longer(mu:prediction) %&gt;% group_by(name) %&gt;% mode_hdi(value) ## # A tibble: 3 x 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 mu 9.65 7.63 12.3 0.95 mode hdi ## 2 prediction 9.57 -1.00 21.0 0.95 mode hdi ## 3 sigma 5.45 3.80 7.09 0.95 mode hdi We can expect 95% of the time to see a wide range of predictions for our data even though the mean and standard deviation are much tighter. Here is a posterior predictive plot for kilo-watt usage in our example for the energy manager. # the simulation set.seed(4242) # the plot samples %&gt;% ggplot(aes(x = prediction)) + geom_histogram(binwidth = 1, center = 0, color = &quot;blue&quot;, alpha = 0.3, size = 1/10) + scale_x_continuous(&quot;usage samples&quot;, breaks = seq(from = 0, to = 50, by = 5)) + scale_y_continuous(NULL, breaks = NULL, limits = c(0, 1200)) + ggtitle(&quot;Posterior predictive distribution&quot;) + coord_cartesian(xlim = c(0, 50)) + theme(panel.grid = element_blank()) Was that enough? For now at least. We accomplished a lot. But perhaps the biggest takeaway is our new found ability to deploy simulation with probability with compatibility of hypotheses with data to arrive at learning about the data. We learned because we inferred. Up next, we gather our wits together to rebuild the workhorse model, the linear regression, in the image and likeness of a probabilistic simulator. Session Information sessionInfo() ## R version 4.0.2 (2020-06-22) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 19041) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=English_United States.1252 ## [2] LC_CTYPE=English_United States.1252 ## [3] LC_MONETARY=English_United States.1252 ## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United States.1252 ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] coda_0.19-3 mvtnorm_1.1-1 tidybayes_2.1.1 ## [4] plotly_4.9.2.1 rethinking_2.12 rstan_2.21.2 ## [7] StanHeaders_2.21.0-6 forcats_0.5.0 stringr_1.4.0 ## [10] dplyr_1.0.1 purrr_0.3.4 readr_1.3.1 ## [13] tidyr_1.1.1 tibble_3.0.3 ggplot2_3.3.2 ## [16] tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] matrixStats_0.56.0 fs_1.5.0 lubridate_1.7.9 ## [4] httr_1.4.2 tools_4.0.2 backports_1.1.7 ## [7] utf8_1.1.4 R6_2.4.1 ggdist_2.2.0 ## [10] DBI_1.1.0 lazyeval_0.2.2 colorspace_1.4-1 ## [13] withr_2.2.0 tidyselect_1.1.0 gridExtra_2.3 ## [16] prettyunits_1.1.1 processx_3.4.3 curl_4.3 ## [19] compiler_4.0.2 cli_2.0.2 rvest_0.3.6 ## [22] Cairo_1.5-12.2 HDInterval_0.2.2 arrayhelpers_1.1-0 ## [25] xml2_1.3.2 labeling_0.3 bookdown_0.20 ## [28] scales_1.1.1 callr_3.4.3 digest_0.6.25 ## [31] rmarkdown_2.3 pkgconfig_2.0.3 htmltools_0.5.0 ## [34] dbplyr_1.4.4 htmlwidgets_1.5.1 rlang_0.4.7 ## [37] readxl_1.3.1 rstudioapi_0.11 svUnit_1.0.3 ## [40] shape_1.4.4 generics_0.0.2 farver_2.0.3 ## [43] jsonlite_1.7.0 crosstalk_1.1.0.1 distributional_0.2.1 ## [46] inline_0.3.15 magrittr_1.5 loo_2.3.1 ## [49] Rcpp_1.0.5 munsell_0.5.0 fansi_0.4.1 ## [52] lifecycle_0.2.0 stringi_1.4.6 yaml_2.2.1 ## [55] MASS_7.3-51.6 plyr_1.8.6 pkgbuild_1.1.0 ## [58] grid_4.0.2 blob_1.2.1 crayon_1.3.4 ## [61] lattice_0.20-41 haven_2.3.1 hms_0.5.3 ## [64] knitr_1.29 ps_1.3.4 pillar_1.4.6 ## [67] codetools_0.2-16 stats4_4.0.2 reprex_0.3.0 ## [70] glue_1.4.1 evaluate_0.14 V8_3.2.0 ## [73] data.table_1.13.0 RcppParallel_5.0.2 modelr_0.1.8 ## [76] vctrs_0.3.2 cellranger_1.1.0 gtable_0.3.0 ## [79] assertthat_0.2.1 xfun_0.16 broom_0.7.0 ## [82] viridisLite_0.3.0 ellipsis_0.3.1 References "]
]
